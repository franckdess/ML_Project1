{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from proj1_helpers import *\n",
    "from functions import *\n",
    "from implementations import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. LOAD THE DATA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the data using the helper function `load_csv_data`. The returned array contains 3 arrays, on for the predictions, one for the features, and one for the ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = load_csv_data('Data/train.csv', sub_sample = True)\n",
    "test_set = load_csv_data('Data/test.csv', sub_sample = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. SET UP THE DATA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We separate the 3 arrays as mentionned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = train_set[1]\n",
    "y_train = train_set[0]\n",
    "ids_train = train_set[2]\n",
    "\n",
    "x_test = test_set[1]\n",
    "y_test = test_set[0]\n",
    "ids_test = test_set[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. CLEAN THE DATA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to our look at the data set, we saw that depending on the `PRI_jet_num` values, some features were undefined for all observations. Hence we decided to split the dataset into 3 subsets depending on the `PRI_jet_num`. This allows us to delete some features without losing any imoportant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" The following separates the initial test set x_test in three subsets\n",
    "    according the feature PRI_jet_num which takes its value in the \n",
    "    set {0,1,2,3} \"\"\"\n",
    "\n",
    "# Concatenating x, y and ids: \n",
    "complete_test = np.column_stack((y_test, x_test))\n",
    "complete_test = np.column_stack((complete_test, ids_test))\n",
    "\n",
    "# Split the data into 3 subsets depending on the value of the feature PRI_jet_num (featute number 23)\n",
    "subset_test_0 = complete_test[complete_test[:,23] == 0]\n",
    "subset_test_1 = complete_test[complete_test[:,23] == 1]\n",
    "subset_test_23 = complete_test[2 <= complete_test[:,23]]\n",
    "\n",
    "# Separate the three subsets to obtain the ids, the features and the prediction\n",
    "y_test_0 = subset_test_0[:,0]\n",
    "y_test_1 = subset_test_1[:,0]\n",
    "y_test_2 = subset_test_23[:,0]\n",
    "\n",
    "x_test_0 = subset_test_0[:,1:-1]\n",
    "x_test_1 = subset_test_1[:,1:-1]\n",
    "x_test_2 = subset_test_23[:,1:-1]\n",
    "\n",
    "id_test_0 = subset_test_0[:,-1]\n",
    "id_test_1 = subset_test_1[:,-1]\n",
    "id_test_2 = subset_test_23[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" The following separates the initial training set x in three subsets\n",
    "    according the feature PRI_jet_num which takes its value in the \n",
    "    set {0,1,2,3} \"\"\"\n",
    "\n",
    "\n",
    "# Concatenating x, y and ids: \n",
    "complete_train = np.column_stack((y_train, x_train))\n",
    "complete_train = np.column_stack((complete_train, ids_train))\n",
    "\n",
    "# Split the data into 3 subsets depending on the value of the feature PRI_jet_num (featute number 23)\n",
    "subset_train_0 = complete_train[complete_train[:,23] == 0]\n",
    "subset_train_1 = complete_train[complete_train[:,23] == 1]\n",
    "subset_train_23 = complete_train[2 <= complete_train[:,23]]\n",
    "\n",
    "# Separate the three subsets to obtain the ids, the features and the prediction\n",
    "y_train_0 = subset_train_0[:,0]\n",
    "y_train_1 = subset_train_1[:,0]\n",
    "y_train_2 = subset_train_23[:,0]\n",
    "\n",
    "x_train_0 = subset_train_0[:,1:-1]\n",
    "x_train_1 = subset_train_1[:,1:-1]\n",
    "x_train_2 = subset_train_23[:,1:-1]\n",
    "\n",
    "id_train_0 = subset_train_0[:,-1]\n",
    "id_train_1 = subset_train_1[:,-1]\n",
    "id_train_2 = subset_train_23[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" The following clean the 3 subsets for the train and test data.\n",
    "    It checks for the undefined values (-999) and for the zeros value\"\"\"\n",
    "\n",
    "# Get the indices of features where the number of undefined values is higher than 95%\n",
    "na_indices_0 = get_na_columns(x_train_0, 0.95, -999)\n",
    "na_indices_1 = get_na_columns(x_train_1, 0.95, -999)\n",
    "na_indices_2 = get_na_columns(x_train_2, 0.95, -999)\n",
    "\n",
    "# Delete those features in the 3 train subsets and in the 3 test subsets\n",
    "x_train_0_clean = np.delete(x_train_0, na_indices_0, axis = 1)\n",
    "x_train_1_clean = np.delete(x_train_1, na_indices_1, axis = 1)\n",
    "x_train_2_clean = np.delete(x_train_2, na_indices_2, axis = 1)\n",
    "\n",
    "x_test_0_clean = np.delete(x_test_0, na_indices_0, axis = 1)\n",
    "x_test_1_clean = np.delete(x_test_1, na_indices_1, axis = 1)\n",
    "x_test_2_clean = np.delete(x_test_2, na_indices_2, axis = 1)\n",
    "\n",
    "# Get the indices of features where the number of zero values is higher than 99%\n",
    "zero_indices_0 = get_na_columns(x_train_0_clean, 0.99, 0.0)\n",
    "zero_indices_1 = get_na_columns(x_train_1_clean, 0.99, 0.0)\n",
    "zero_indices_2 = get_na_columns(x_train_2_clean, 0.99, 0.0)\n",
    "\n",
    "# Delete those features in the 3 train subsets and in the 3 test subsets\n",
    "x_train_0_clean = np.delete(x_train_0_clean, zero_indices_0, axis = 1)\n",
    "x_train_1_clean = np.delete(x_train_1_clean, zero_indices_1, axis = 1)\n",
    "x_train_2_clean = np.delete(x_train_2_clean, zero_indices_2, axis = 1)\n",
    "\n",
    "x_test_0_clean = np.delete(x_test_0_clean, zero_indices_0, axis = 1)\n",
    "x_test_1_clean = np.delete(x_test_1_clean, zero_indices_1, axis = 1)\n",
    "x_test_2_clean = np.delete(x_test_2_clean, zero_indices_2, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For the 3 train subsets, get the indices of the features where it stays any undefined value\n",
    "na_indices_0_rem = get_na_columns(x_train_0_clean, 0, -999)\n",
    "na_indices_1_rem = get_na_columns(x_train_1_clean, 0, -999)\n",
    "na_indices_2_rem = get_na_columns(x_train_2_clean, 0, -999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find a model to predict the undefined values using least squares\n",
    "x_train_0_clean_pred, w_train_0 = predict_na_columns(x_train_0_clean, na_indices_0_rem)\n",
    "x_train_1_clean_pred, w_train_1 = predict_na_columns(x_train_1_clean, na_indices_1_rem)\n",
    "x_train_2_clean_pred, w_train_2 = predict_na_columns(x_train_2_clean, na_indices_2_rem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict the undefined value in the test set using the model obtained with the train set\n",
    "x_test_0_clean_pred = set_predict_na_columns(x_test_0_clean, w_train_0[0], na_indices_0_rem)\n",
    "x_test_1_clean_pred = set_predict_na_columns(x_test_1_clean, w_train_1[0], na_indices_1_rem)\n",
    "x_test_2_clean_pred = set_predict_na_columns(x_test_2_clean, w_train_2[0], na_indices_2_rem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standardize all train and test subsets. and add the intercept at the beginning\n",
    "x_train_0_std, x_test_0_std = standardize(x_train_0_clean_pred, x_test_0_clean_pred)\n",
    "x_train_0_std_int = np.column_stack((np.ones(x_train_0_std.shape[0]), x_train_0_std))\n",
    "x_test_0_std_int = np.column_stack((np.ones(x_test_0_std.shape[0]), x_test_0_std))\n",
    "\n",
    "x_train_1_std, x_test_1_std = standardize(x_train_1_clean_pred, x_test_1_clean_pred)\n",
    "x_train_1_std_int = np.column_stack((np.ones(x_train_1_std.shape[0]), x_train_1_std))\n",
    "x_test_1_std_int = np.column_stack((np.ones(x_test_1_std.shape[0]), x_test_1_std))\n",
    "\n",
    "x_train_2_std, x_test_2_std = standardize(x_train_2_clean_pred, x_test_2_clean_pred)\n",
    "x_train_2_std_int = np.column_stack((np.ones(x_train_2_std.shape[0]), x_train_2_std))\n",
    "x_test_2_std_int = np.column_stack((np.ones(x_test_2_std.shape[0]), x_test_2_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. GET THE MODEL**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the model using ridge regression function with parameter lambda = $10^{-9}$, and a polynomial expansion of degree 3. Those parameters where obtained by previous computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Polynomial expansion of degree 3 for the 3 train subsets\n",
    "x_train_0_2 = build_poly(x_train_0_std_int, degree = 3)\n",
    "x_train_1_2 = build_poly(x_train_1_std_int, degree = 3)\n",
    "x_train_2_2 = build_poly(x_train_2_std_int, degree = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the model and the loss using ridge regression\n",
    "w_0, loss_0 = ridge_regression(y_train_0, x_train_0_2, 10**(-11))\n",
    "w_1, loss_1 = ridge_regression(y_train_1, x_train_1_2, 10**(-11))\n",
    "w_2, loss_2 = ridge_regression(y_train_2, x_train_2_2, 10**(-11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. PREDICT Y**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we predict the y's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Polynomial expansion of degree 3 for the 3 test subsets\n",
    "x_test_0_2 = build_poly(x_test_0_std_int, degree = 3)\n",
    "x_test_1_2 = build_poly(x_test_1_std_int, degree = 3)\n",
    "x_test_2_2 = build_poly(x_test_2_std_int, degree = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the prediction of y, bring back the values into the interval [0, 1] \n",
    "# using sigmoid function and transform the zero values into -1\n",
    "y_0 = zero_to_neg(np.around(sigmoid(x_test_0_2 @ w_0)))\n",
    "y_1 = zero_to_neg(np.around(sigmoid(x_test_1_2 @ w_1)))\n",
    "y_2 = zero_to_neg(np.around(sigmoid(x_test_2_2 @ w_2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build back the predictions vector, concatenated with the ids\n",
    "s_0 = np.column_stack((id_test_0, y_0))\n",
    "s_1 = np.column_stack((id_test_1, y_1))\n",
    "s_2 = np.column_stack((id_test_2, y_2))\n",
    "s = np.vstack((np.vstack((s_0, s_1)), s_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sort the predicton by their id\n",
    "y_pred = s[s[:,0].argsort()].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. SAVE FILE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_csv_submission(y_pred[:,0], y_pred[:, 1], 'final_prediction.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
