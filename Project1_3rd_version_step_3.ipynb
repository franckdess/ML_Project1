{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from proj1_helpers import load_csv_data\n",
    "from functions import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. LOAD THE DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set = load_csv_data('Data/train.csv', sub_sample = False)\n",
    "test_set = load_csv_data('Data/test.csv', sub_sample = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. SET UP THE DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = train_set[1]\n",
    "y = train_set[0]\n",
    "ids = train_set[2]\n",
    "\n",
    "x_train, x_test, y_train, y_test, ids_train, ids_test = split_data_tr_te(x, y, ids, 0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x_train = train_set[1]\n",
    "y_train = train_set[0]\n",
    "ids_train = train_set[2]\n",
    "\n",
    "x_test = test_set[1]\n",
    "y_test = test_set[0]\n",
    "ids_test = test_set[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The following separete the initial test set x_test in three subsets\n",
    "# according the feature PRI_jet_num which takes its value in the \n",
    "# set {0,1,2,3}\n",
    "\n",
    "\n",
    "# Concatenating x, y and ids: \n",
    "\n",
    "complete_test = np.column_stack((y_test, x_test))\n",
    "complete_test = np.column_stack((complete_test, ids_test))\n",
    "\n",
    "# Creating three subsets by the value of the feature PRI_jet_num \n",
    "# which is column 23\n",
    "\n",
    "\n",
    "# We create 3 sub arrays (Subset-0, Subset-1, Subset-23) \n",
    "# according the value of Pri_jet_num which is feature 23\n",
    "\n",
    "subset_test_0 = complete_test[complete_test[:,23] == 0]\n",
    "subset_test_1 = complete_test[complete_test[:,23] == 1]\n",
    "subset_test_23 = complete_test[2 <= complete_test[:,23]]\n",
    "\n",
    "# Separate the subsets by ids_test, y_test and x_test\n",
    "\n",
    "y_test_0 = subset_test_0[:,0]\n",
    "y_test_1 = subset_test_1[:,0]\n",
    "y_test_2 = subset_test_23[:,0]\n",
    "\n",
    "x_test_0 = subset_test_0[:,1:-1]\n",
    "x_test_1 = subset_test_1[:,1:-1]\n",
    "x_test_2 = subset_test_23[:,1:-1]\n",
    "\n",
    "id_test_0 = subset_test_0[:,-1]\n",
    "id_test_1 = subset_test_1[:,-1]\n",
    "id_test_2 = subset_test_23[:,-1]\n",
    "\n",
    "\n",
    "\n",
    "# The following separete the initial training set x in three subsets\n",
    "# according the feature PRI_jet_num which takes its value in the \n",
    "# set {0,1,2,3}\n",
    "\n",
    "\n",
    "# Concatenating x, y and ids: \n",
    "\n",
    "complete_train = np.column_stack((y_train, x_train))\n",
    "complete_train = np.column_stack((complete_train, ids_train))\n",
    "\n",
    "# Creating three subsets by the value of the feature PRI_jet_num \n",
    "# which is column 23\n",
    "\n",
    "\n",
    "# We create 3 sub arrays (Subset-0, Subset-1, Subset-23) \n",
    "# according the value of Pri_jet_num which is feature 23\n",
    "\n",
    "subset_train_0 = complete_train[complete_train[:,23] == 0]\n",
    "subset_train_1 = complete_train[complete_train[:,23] == 1]\n",
    "subset_train_23 = complete_train[2 <= complete_train[:,23]]\n",
    "\n",
    "# Separate the subsets by ids, y and x\n",
    "\n",
    "y_train_0 = subset_train_0[:,0]\n",
    "y_train_1 = subset_train_1[:,0]\n",
    "y_train_2 = subset_train_23[:,0]\n",
    "\n",
    "x_train_0 = subset_train_0[:,1:-1]\n",
    "x_train_1 = subset_train_1[:,1:-1]\n",
    "x_train_2 = subset_train_23[:,1:-1]\n",
    "\n",
    "id_train_0 = subset_train_0[:,-1]\n",
    "id_train_1 = subset_train_1[:,-1]\n",
    "id_train_2 = subset_train_23[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_0_std, x_test_0_std = standardize(x_train_0, x_test_0)\n",
    "x_train_0_std_int = np.column_stack((np.ones(x_train_0_std.shape[0]), x_train_0_std))\n",
    "x_test_0_std_int = np.column_stack((np.ones(x_test_0_std.shape[0]), x_test_0_std))\n",
    "\n",
    "x_train_1_std, x_test_1_std = standardize(x_train_1, x_test_1)\n",
    "x_train_1_std_int = np.column_stack((np.ones(x_train_1_std.shape[0]), x_train_1_std))\n",
    "x_test_1_std_int = np.column_stack((np.ones(x_test_1_std.shape[0]), x_test_1_std))\n",
    "\n",
    "x_train_2_std, x_test_2_std = standardize(x_train_2, x_test_2)\n",
    "x_train_2_std_int = np.column_stack((np.ones(x_train_2_std.shape[0]), x_train_2_std))\n",
    "x_test_2_std_int = np.column_stack((np.ones(x_test_2_std.shape[0]), x_test_2_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. CLEAN THE DATA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. DEFINE THE FUNCTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    A = tx.T@tx\n",
    "    b = tx.T@y\n",
    "    w = np.linalg.solve(A, b)\n",
    "    loss = compute_mse(y, tx, w)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    ws = [initial_w]\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        gradient = compute_gradient_least_square(y, tx, w)\n",
    "        loss = compute_mse(y, tx, w)\n",
    "        w = w - gamma*gradient\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def least_squares_SGD(y, tx, initial_w, max_iters, gamma):\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):    \n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size):\n",
    "            gradient = compute_stoch_gradient(minibatch_y, minibatch_tx, w)\n",
    "            loss = compute_mse(minibatch_y, minibatch_tx, w)\n",
    "            new_w = w - gamma*gradient\n",
    "            w = new_w\n",
    "            ws.append(w)\n",
    "            losses.append(loss)\n",
    "            print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "            bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return ws, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lamb):\n",
    "    \"\"\"implement ridge regression.\"\"\"\n",
    "    aI = lamb * np.identity(tx.shape[1])\n",
    "    a = tx.T.dot(tx) + aI\n",
    "    b = tx.T.dot(y)\n",
    "    w = np.linalg.solve(a, b)\n",
    "    loss = compute_mse(y, tx, w)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression(y, tx, initial_w, max_iters, gamma):\n",
    "    loss, w = gradient_descent_log_reg(y, tx, initial_w, max_iters, gamma)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma):\n",
    "    loss, w = reg_gradient_descent_log_reg(y, tx, lambda_, initial_w, max_iters, gamma)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.CROSS VALIDATION VIZUALIZATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cross_validation_demo(y_train_0, x_train_0_std_int, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cross_validation_demo(y_train_1, x_train_1_std_int, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cross_validation_demo(y_train_2, x_train_2_std_int, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cross_validation_demo(y_train_0, x_train_0_std_int, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cross_validation_demo(y_train, x_train_std_int_, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cross_validation_demo(y_train, x_train_std_int_, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cross_validation_demo(y_train, x_train_std_int_, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. GET THE MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_0_2 = build_poly(x_train_0_std_int, degree = 2)\n",
    "x_train_1_2 = build_poly(x_train_1_std_int, degree = 2)\n",
    "x_train_2_2 = build_poly(x_train_2_std_int, degree = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_0, loss_0 = ridge_regression(y_train_0, x_train_0_2, 10**(-12))\n",
    "w_1, loss_1 = ridge_regression(y_train_1, x_train_1_2, 10**(-12))\n",
    "w_2, loss_2 = ridge_regression(y_train_2, x_train_2_2, 10**(-12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test_0_2 = build_poly(x_test_0_std_int, degree = 2)\n",
    "x_test_1_2 = build_poly(x_test_1_std_int, degree = 2)\n",
    "x_test_2_2 = build_poly(x_test_2_std_int, degree = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_0 = zero_to_neg(np.around(sigmoid(x_test_0_2 @ w_0)))\n",
    "y_1 = zero_to_neg(np.around(sigmoid(x_test_1_2 @ w_1)))\n",
    "y_2 = zero_to_neg(np.around(sigmoid(x_test_2_2 @ w_2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s_0 = np.column_stack((id_test_0, y_0))\n",
    "s_1 = np.column_stack((id_test_1, y_1))\n",
    "s_2 = np.column_stack((id_test_2, y_2))\n",
    "s = np.vstack((np.vstack((s_0, s_1)), s_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ss = s[s[:,0].argsort()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = np.column_stack((ids_test, y_test))\n",
    "test = test[test[:, 0].argsort()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_pred(y_test, y_pred):\n",
    "    sum_id = np.sum(np.abs(y_test[:, 0] - y_pred[:, 0]))\n",
    "    acc = 1 - np.sum(np.abs(y_test[:, 1] - y_pred[:, 1])) / y_test.shape[0] * 0.5\n",
    "    return sum_id, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sum_id, acc = test_pred(test, ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 0.76867294117647056)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_id, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "s = np.column_stack((ids_test, ss_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "s_df = pd.DataFrame(ss)\n",
    "s_df.columns = ['Id', 'Prediction']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "len(s_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "s_df.to_csv('Data/25_submit_prediction_rid_reg_deg_3_10-9_3_subsets_std_int.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
