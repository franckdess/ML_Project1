{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from proj1_helpers import *\n",
    "from functions import *\n",
    "from implementations import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. LOAD THE DATA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the data using the helper function `load_csv_data`. The returned array contains 3 arrays, on for the predictions, one for the features, and one for the ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Data/train.csv not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-94b861f6b342>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_csv_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Data/train.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_csv_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Data/test.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/franckdessimoz/Desktop/Data Science - MA1/Machine Learning/ML_project1/Submission/proj1_helpers.py\u001b[0m in \u001b[0;36mload_csv_data\u001b[0;34m(data_path, sub_sample)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_csv_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;34m\"\"\"Loads data and returns y (class labels), tX (features) and ids (event ids)\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_header\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_header\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/franckdessimoz/anaconda/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mgenfromtxt\u001b[0;34m(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows)\u001b[0m\n\u001b[1;32m   1510\u001b[0m                 \u001b[0mfhd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rbU'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1511\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1512\u001b[0;31m                 \u001b[0mfhd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1513\u001b[0m             \u001b[0mown_fhd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1514\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/franckdessimoz/anaconda/lib/python3.6/site-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(path, mode, destpath)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/franckdessimoz/anaconda/lib/python3.6/site-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode)\u001b[0m\n\u001b[1;32m    499\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_file_openers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s not found.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Data/train.csv not found."
     ]
    }
   ],
   "source": [
    "train_set = load_csv_data('Data/train.csv', sub_sample = True)\n",
    "test_set = load_csv_data('Data/test.csv', sub_sample = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. SET UP THE DATA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We separate the 3 arrays as mentionned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = train_set[1]\n",
    "y_train = train_set[0]\n",
    "ids_train = train_set[2]\n",
    "\n",
    "x_test = test_set[1]\n",
    "y_test = test_set[0]\n",
    "ids_test = test_set[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. CLEAN THE DATA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to our look at the data set, we saw that depending on the `PRI_jet_num` values, some features were undefined for all observations. Hence we decided to split the dataset into 3 subsets depending on the `PRI_jet_num`. This allows us to delete some features without losing any imoportant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" The following separates the initial test set x_test in three subsets\n",
    "    according the feature PRI_jet_num which takes its value in the \n",
    "    set {0,1,2,3} \"\"\"\n",
    "\n",
    "# Concatenating x, y and ids: \n",
    "complete_test = np.column_stack((y_test, x_test))\n",
    "complete_test = np.column_stack((complete_test, ids_test))\n",
    "\n",
    "# Split the data into 3 subsets depending on the value of the feature PRI_jet_num (featute number 23)\n",
    "subset_test_0 = complete_test[complete_test[:,23] == 0]\n",
    "subset_test_1 = complete_test[complete_test[:,23] == 1]\n",
    "subset_test_23 = complete_test[2 <= complete_test[:,23]]\n",
    "\n",
    "# Separate the three subsets to obtain the ids, the features and the prediction\n",
    "y_test_0 = subset_test_0[:,0]\n",
    "y_test_1 = subset_test_1[:,0]\n",
    "y_test_2 = subset_test_23[:,0]\n",
    "\n",
    "x_test_0 = subset_test_0[:,1:-1]\n",
    "x_test_1 = subset_test_1[:,1:-1]\n",
    "x_test_2 = subset_test_23[:,1:-1]\n",
    "\n",
    "id_test_0 = subset_test_0[:,-1]\n",
    "id_test_1 = subset_test_1[:,-1]\n",
    "id_test_2 = subset_test_23[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" The following separates the initial training set x in three subsets\n",
    "    according the feature PRI_jet_num which takes its value in the \n",
    "    set {0,1,2,3} \"\"\"\n",
    "\n",
    "\n",
    "# Concatenating x, y and ids: \n",
    "complete_train = np.column_stack((y_train, x_train))\n",
    "complete_train = np.column_stack((complete_train, ids_train))\n",
    "\n",
    "# Split the data into 3 subsets depending on the value of the feature PRI_jet_num (featute number 23)\n",
    "subset_train_0 = complete_train[complete_train[:,23] == 0]\n",
    "subset_train_1 = complete_train[complete_train[:,23] == 1]\n",
    "subset_train_23 = complete_train[2 <= complete_train[:,23]]\n",
    "\n",
    "# Separate the three subsets to obtain the ids, the features and the prediction\n",
    "y_train_0 = subset_train_0[:,0]\n",
    "y_train_1 = subset_train_1[:,0]\n",
    "y_train_2 = subset_train_23[:,0]\n",
    "\n",
    "x_train_0 = subset_train_0[:,1:-1]\n",
    "x_train_1 = subset_train_1[:,1:-1]\n",
    "x_train_2 = subset_train_23[:,1:-1]\n",
    "\n",
    "id_train_0 = subset_train_0[:,-1]\n",
    "id_train_1 = subset_train_1[:,-1]\n",
    "id_train_2 = subset_train_23[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" The following clean the 3 subsets for the train and test data.\n",
    "    It checks for the undefined values (-999) and for the zeros value\"\"\"\n",
    "\n",
    "# Get the indices of features where the number of undefined values is higher than 95%\n",
    "na_indices_0 = get_na_columns(x_train_0, 0.95, -999)\n",
    "na_indices_1 = get_na_columns(x_train_1, 0.95, -999)\n",
    "na_indices_2 = get_na_columns(x_train_2, 0.95, -999)\n",
    "\n",
    "# Delete those features in the 3 train subsets and in the 3 test subsets\n",
    "x_train_0_clean = np.delete(x_train_0, na_indices_0, axis = 1)\n",
    "x_train_1_clean = np.delete(x_train_1, na_indices_1, axis = 1)\n",
    "x_train_2_clean = np.delete(x_train_2, na_indices_2, axis = 1)\n",
    "\n",
    "x_test_0_clean = np.delete(x_test_0, na_indices_0, axis = 1)\n",
    "x_test_1_clean = np.delete(x_test_1, na_indices_1, axis = 1)\n",
    "x_test_2_clean = np.delete(x_test_2, na_indices_2, axis = 1)\n",
    "\n",
    "# Get the indices of features where the number of zero values is higher than 99%\n",
    "zero_indices_0 = get_na_columns(x_train_0_clean, 0.99, 0.0)\n",
    "zero_indices_1 = get_na_columns(x_train_1_clean, 0.99, 0.0)\n",
    "zero_indices_2 = get_na_columns(x_train_2_clean, 0.99, 0.0)\n",
    "\n",
    "# Delete those features in the 3 train subsets and in the 3 test subsets\n",
    "x_train_0_clean = np.delete(x_train_0_clean, zero_indices_0, axis = 1)\n",
    "x_train_1_clean = np.delete(x_train_1_clean, zero_indices_1, axis = 1)\n",
    "x_train_2_clean = np.delete(x_train_2_clean, zero_indices_2, axis = 1)\n",
    "\n",
    "x_test_0_clean = np.delete(x_test_0_clean, zero_indices_0, axis = 1)\n",
    "x_test_1_clean = np.delete(x_test_1_clean, zero_indices_1, axis = 1)\n",
    "x_test_2_clean = np.delete(x_test_2_clean, zero_indices_2, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For the 3 train subsets, get the indices of the features where it stays any undefined value\n",
    "na_indices_0_rem = get_na_columns(x_train_0_clean, 0, -999)\n",
    "na_indices_1_rem = get_na_columns(x_train_1_clean, 0, -999)\n",
    "na_indices_2_rem = get_na_columns(x_train_2_clean, 0, -999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find a model to predict the undefined values using least squares\n",
    "x_train_0_clean_pred, w_train_0 = predict_na_columns(x_train_0_clean, na_indices_0_rem)\n",
    "x_train_1_clean_pred, w_train_1 = predict_na_columns(x_train_1_clean, na_indices_1_rem)\n",
    "x_train_2_clean_pred, w_train_2 = predict_na_columns(x_train_2_clean, na_indices_2_rem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict the undefined value in the test set using the model obtained with the train set\n",
    "x_test_0_clean_pred = set_predict_na_columns(x_test_0_clean, w_train_0[0], na_indices_0_rem)\n",
    "x_test_1_clean_pred = set_predict_na_columns(x_test_1_clean, w_train_1[0], na_indices_1_rem)\n",
    "x_test_2_clean_pred = set_predict_na_columns(x_test_2_clean, w_train_2[0], na_indices_2_rem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standardize all train and test subsets. and add the intercept at the beginning\n",
    "x_train_0_std, x_test_0_std = standardize(x_train_0_clean_pred, x_test_0_clean_pred)\n",
    "x_train_0_std_int = np.column_stack((np.ones(x_train_0_std.shape[0]), x_train_0_std))\n",
    "x_test_0_std_int = np.column_stack((np.ones(x_test_0_std.shape[0]), x_test_0_std))\n",
    "\n",
    "x_train_1_std, x_test_1_std = standardize(x_train_1_clean_pred, x_test_1_clean_pred)\n",
    "x_train_1_std_int = np.column_stack((np.ones(x_train_1_std.shape[0]), x_train_1_std))\n",
    "x_test_1_std_int = np.column_stack((np.ones(x_test_1_std.shape[0]), x_test_1_std))\n",
    "\n",
    "x_train_2_std, x_test_2_std = standardize(x_train_2_clean_pred, x_test_2_clean_pred)\n",
    "x_train_2_std_int = np.column_stack((np.ones(x_train_2_std.shape[0]), x_train_2_std))\n",
    "x_test_2_std_int = np.column_stack((np.ones(x_test_2_std.shape[0]), x_test_2_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. GET THE MODEL**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the model using ridge regression function with parameter lambda = $10^{-9}$, and a polynomial expansion of degree 3. Those parameters where obtained by previous computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Polynomial expansion of degree 3 for the 3 train subsets\n",
    "x_train_0_2 = build_poly(x_train_0_std_int, degree = 3)\n",
    "x_train_1_2 = build_poly(x_train_1_std_int, degree = 3)\n",
    "x_train_2_2 = build_poly(x_train_2_std_int, degree = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the model and the loss using ridge regression\n",
    "w_0, loss_0 = ridge_regression(y_train_0, x_train_0_2, 10**(-9))\n",
    "w_1, loss_1 = ridge_regression(y_train_1, x_train_1_2, 10**(-9))\n",
    "w_2, loss_2 = ridge_regression(y_train_2, x_train_2_2, 10**(-9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. PREDICT Y**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we predict the y's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Polynomial expansion of degree 3 for the 3 test subsets\n",
    "x_test_0_2 = build_poly(x_test_0_std_int, degree = 3)\n",
    "x_test_1_2 = build_poly(x_test_1_std_int, degree = 3)\n",
    "x_test_2_2 = build_poly(x_test_2_std_int, degree = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the prediction of y, bring back the values into the interval [0, 1] \n",
    "# using sigmoid function and transform the zero values into -1\n",
    "y_0 = zero_to_neg(np.around(sigmoid(x_test_0_2 @ w_0)))\n",
    "y_1 = zero_to_neg(np.around(sigmoid(x_test_1_2 @ w_1)))\n",
    "y_2 = zero_to_neg(np.around(sigmoid(x_test_2_2 @ w_2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build back the predictions vector, concatenated with the ids\n",
    "s_0 = np.column_stack((id_test_0, y_0))\n",
    "s_1 = np.column_stack((id_test_1, y_1))\n",
    "s_2 = np.column_stack((id_test_2, y_2))\n",
    "s = np.vstack((np.vstack((s_0, s_1)), s_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sort the predicton by their id\n",
    "y_pred = s[s[:,0].argsort()].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. SAVE FILE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_csv_submission(y_pred[:,0], y_pred[:, 1], 'final_prediction.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
