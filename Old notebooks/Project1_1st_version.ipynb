{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from proj1_helpers import load_csv_data\n",
    "from functions import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.   SETUP THE DATA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create pandas DataFrames to clear the train data and the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set_pd = pd.read_csv('Data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_set_pd = pd.read_csv('Data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate the data depending on the columns used which can be found by looking at the parameter 'PRI_jet_num'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set_pd_0 = train_set_pd[train_set_pd.PRI_jet_num == 0].drop(['DER_deltaeta_jet_jet', 'DER_mass_jet_jet', 'DER_prodeta_jet_jet', 'DER_lep_eta_centrality', 'PRI_jet_leading_pt', 'PRI_jet_leading_eta', 'PRI_jet_leading_phi', 'PRI_jet_subleading_pt', 'PRI_jet_subleading_eta', 'PRI_jet_subleading_phi', 'PRI_jet_num'], axis = 1)\n",
    "train_set_pd_1 = train_set_pd[train_set_pd.PRI_jet_num == 1].drop(['DER_deltaeta_jet_jet', 'DER_mass_jet_jet', 'DER_prodeta_jet_jet', 'DER_lep_eta_centrality', 'PRI_jet_subleading_pt', 'PRI_jet_subleading_eta', 'PRI_jet_subleading_phi', 'PRI_jet_num'], axis = 1)\n",
    "train_set_pd_2 = train_set_pd[train_set_pd.PRI_jet_num.isin([2, 3])].drop(['PRI_jet_num'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_set_pd_0 = test_set_pd[test_set_pd.PRI_jet_num == 0].drop(['DER_deltaeta_jet_jet', 'DER_mass_jet_jet', 'DER_prodeta_jet_jet', 'DER_lep_eta_centrality', 'PRI_jet_leading_pt', 'PRI_jet_leading_eta', 'PRI_jet_leading_phi', 'PRI_jet_subleading_pt', 'PRI_jet_subleading_eta', 'PRI_jet_subleading_phi', 'PRI_jet_num'], axis = 1)\n",
    "test_set_pd_1 = test_set_pd[test_set_pd.PRI_jet_num == 1].drop(['DER_deltaeta_jet_jet', 'DER_mass_jet_jet', 'DER_prodeta_jet_jet', 'DER_lep_eta_centrality', 'PRI_jet_subleading_pt', 'PRI_jet_subleading_eta', 'PRI_jet_subleading_phi', 'PRI_jet_num'], axis = 1)\n",
    "test_set_pd_2 = test_set_pd[test_set_pd.PRI_jet_num.isin([2, 3])].drop(['PRI_jet_num'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We replace the -999 by the mean of the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_0_tr = get_column_mean(-999.000, train_set_pd_0, 'DER_mass_MMC')\n",
    "mean_1_tr = get_column_mean(-999.000, train_set_pd_1, 'DER_mass_MMC')\n",
    "mean_2_tr = get_column_mean(-999.000, train_set_pd_2, 'DER_mass_MMC')\n",
    "\n",
    "train_set_pd_0['DER_mass_MMC'] = train_set_pd_0['DER_mass_MMC'].apply(lambda x: replace_by_mean(x, mean_0_tr, 'DER_mass_MMC'))\n",
    "train_set_pd_1['DER_mass_MMC'] = train_set_pd_1['DER_mass_MMC'].apply(lambda x: replace_by_mean(x, mean_1_tr, 'DER_mass_MMC'))\n",
    "train_set_pd_2['DER_mass_MMC'] = train_set_pd_2['DER_mass_MMC'].apply(lambda x: replace_by_mean(x, mean_2_tr, 'DER_mass_MMC'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_0_te = get_column_mean(-999.000, test_set_pd_0, 'DER_mass_MMC')\n",
    "mean_1_te = get_column_mean(-999.000, test_set_pd_1, 'DER_mass_MMC')\n",
    "mean_2_te = get_column_mean(-999.000, test_set_pd_2, 'DER_mass_MMC')\n",
    "\n",
    "test_set_pd_0['DER_mass_MMC'] = test_set_pd_0['DER_mass_MMC'].apply(lambda x: replace_by_mean(x, mean_0_te, 'DER_mass_MMC'))\n",
    "test_set_pd_1['DER_mass_MMC'] = test_set_pd_1['DER_mass_MMC'].apply(lambda x: replace_by_mean(x, mean_1_te, 'DER_mass_MMC'))\n",
    "test_set_pd_2['DER_mass_MMC'] = test_set_pd_2['DER_mass_MMC'].apply(lambda x: replace_by_mean(x, mean_2_te, 'DER_mass_MMC'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the three DataFrames then we can open them with the load_csv function, which returns usable tuples of y's, x's and ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set_pd_0.to_csv('Data/train_set_0.csv', index = False)\n",
    "train_set_pd_1.to_csv('Data/train_set_1.csv', index = False)\n",
    "train_set_pd_2.to_csv('Data/train_set_2.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_set_pd_0.to_csv('Data/test_set_0.csv', index = False)\n",
    "test_set_pd_1.to_csv('Data/test_set_1.csv', index = False)\n",
    "test_set_pd_2.to_csv('Data/test_set_2.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set_0 = load_csv_data('Data/train_set_0.csv')\n",
    "train_set_1 = load_csv_data('Data/train_set_1.csv')\n",
    "train_set_2 = load_csv_data('Data/train_set_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_set_0 = load_csv_data('Data/test_set_0.csv')\n",
    "test_set_1 = load_csv_data('Data/test_set_1.csv')\n",
    "test_set_2 = load_csv_data('Data/test_set_2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We separate x's the y's and the id's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_0 = train_set_0[1]\n",
    "x_train_1 = train_set_1[1]\n",
    "x_train_2 = train_set_2[1]\n",
    "\n",
    "y_train_0 = train_set_0[0]\n",
    "y_train_1 = train_set_1[0]\n",
    "y_train_2 = train_set_2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test_0 = test_set_0[1]\n",
    "x_test_1 = test_set_1[1]\n",
    "x_test_2 = test_set_2[1]\n",
    "\n",
    "id_test_0 = test_set_0[2]\n",
    "id_test_1 = test_set_1[2]\n",
    "id_test_2 = test_set_2[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We standardize all observations and add the intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Standardized version\n",
    "x_train_0 = (x_train_0.T - x_train_0.mean(1)).T\n",
    "y_0 = x_train_0.std(1)\n",
    "x_train_0 = (x_train_0.T/y_0).T\n",
    "x_train_0 = np.column_stack((np.ones(len(x_train_0)), x_train_0))\n",
    "\n",
    "x_train_1 = (x_train_1.T - x_train_1.mean(1)).T\n",
    "y_1 = x_train_1.std(1)\n",
    "x_train_1 = (x_train_1.T/y_1).T\n",
    "x_train_1 = np.column_stack((np.ones(len(x_train_1)), x_train_1))\n",
    "\n",
    "x_train_2 = (x_train_2.T - x_train_2.mean(1)).T\n",
    "y_2 = x_train_2.std(1)\n",
    "x_train_2 = (x_train_2.T/y_2).T\n",
    "x_train_2 = np.column_stack((np.ones(len(x_train_2)), x_train_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Standardized version\n",
    "x_test_0 = (x_test_0.T - x_test_0.mean(1)).T\n",
    "y_te_0 = x_test_0.std(1)\n",
    "x_test_0 = (x_test_0.T/y_te_0).T\n",
    "x_test_0 = np.column_stack((np.ones(len(x_test_0)), x_test_0))\n",
    "\n",
    "x_test_1 = (x_test_1.T - x_test_1.mean(1)).T\n",
    "y_te_1 = x_test_1.std(1)\n",
    "x_test_1 = (x_test_1.T/y_te_1).T\n",
    "x_test_1 = np.column_stack((np.ones(len(x_test_1)), x_test_1))\n",
    "\n",
    "x_test_2 = (x_test_2.T - x_test_2.mean(1)).T\n",
    "y_te_2 = x_test_2.std(1)\n",
    "x_test_2 = (x_test_2.T/y_te_2).T\n",
    "x_test_2 = np.column_stack((np.ones(len(x_test_2)), x_test_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We replace all \"-1\" prediction by \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train_0 = neg_to_zero(y_train_0)\n",
    "y_train_1 = neg_to_zero(y_train_1)\n",
    "y_train_2 = neg_to_zero(y_train_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. DEFINE ALL FUNCTION **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    ws = [initial_w]\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        gradient = compute_gradient_least_square(y, tx, w)\n",
    "        loss = compute_mse(y, tx, w)\n",
    "        w = w - gamma*gradient\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def least_squares_SGD(y, tx, initial_w, max_iters, gamma):\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):    \n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size):\n",
    "            gradient = compute_stoch_gradient(minibatch_y, minibatch_tx, w)\n",
    "            loss = compute_mse(minibatch_y, minibatch_tx, w)\n",
    "            new_w = w - gamma*gradient\n",
    "            w = new_w\n",
    "            ws.append(w)\n",
    "            losses.append(loss)\n",
    "            print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "            bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return ws[len(ws)-1], losss[len(losses)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    A = tx.T@tx\n",
    "    b = tx.T@y\n",
    "    w = np.linalg.solve(A, b)\n",
    "    loss = compute_mse(y, tx, w)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_):\n",
    "    X = tx.T@tx\n",
    "    N = X.shape[0]\n",
    "    A = (X + 2*N*lambda_*np.identity(N))\n",
    "    b = tx.T@y\n",
    "    w = np.linalg.solve(A, b)\n",
    "    loss = compute_mse(y, tx, w)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression(y, tx, initial_w, max_iters, gamma):\n",
    "    loss, w = gradient_descent_log_reg(y, tx, initial_w, max_iters, gamma)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma):\n",
    "    loss, w = reg_gradient_descent_log_reg(y, tx, lambda_, initial_w, max_iters, gamma)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**3. GET THE MODEL**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make some tests to find the correct gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def cross_validation_visualization(lambds, mse_tr, mse_te):\n",
    "    \"\"\"visualization the curves of mse_tr and mse_te.\"\"\"\n",
    "    plt.semilogx(lambds, mse_tr, marker=\".\", color='b', label='train error')\n",
    "    plt.semilogx(lambds, mse_te, marker=\".\", color='r', label='test error')\n",
    "    plt.xlabel(\"lambda\")\n",
    "    plt.ylabel(\"rmse\")\n",
    "    plt.title(\"cross validation\")\n",
    "    plt.legend(loc=2)\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validation(y, x, k_indices, k, lambda_):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    # get k'th subgroup in test, others in train: TODO\n",
    "    \n",
    "    test_indices = k_indices[k]\n",
    "    train_indices = np.concatenate((k_indices[:k], k_indices[k+1:])).flatten()\n",
    "    \n",
    "    x_train = x[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    x_test = x[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    \n",
    "    # ridge regression: TODO\n",
    "    w_train, loss = logistic_regression(y_train, x_train, np.zeros(x_train.shape[1]), 1000, lambda_)\n",
    "    \n",
    "    # ***************************************************\n",
    "    # calculate the loss for train and test data: TODO\n",
    "    loss_tr = compute_mse(y_train, x_train, w_train)\n",
    "    loss_te = compute_mse(y_test, x_test, w_train)\n",
    "\n",
    "    return loss_tr, loss_te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def cross_validation_demo():\n",
    "    seed = 1\n",
    "    degree = 7\n",
    "    k_fold = 4\n",
    "    lambdas = np.logspace(-5, 0, 30)\n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y_train_2, k_fold, seed)\n",
    "    # define lists to store the loss of training data and test data\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    # ***************************************************\n",
    "    for lambda_ in lambdas:\n",
    "        err_train = []\n",
    "        err_test = []\n",
    "        for k in range (k_fold):\n",
    "            loss_tr, loss_te = cross_validation(y_train_2, x_train_2, k_indices, k, lambda_) \n",
    "            err_train.append(np.sqrt(2*loss_tr))\n",
    "            err_test.append(np.sqrt(2*loss_te))\n",
    "        rmse_tr.append(sum(err_train)/k_fold)\n",
    "        rmse_te.append(sum(err_test)/k_fold)\n",
    "    # ***************************************************    \n",
    "    cross_validation_visualization(lambdas, rmse_tr, rmse_te)\n",
    "\n",
    "cross_validation_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the model and the log for the 3 different sets of observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_0, loss_0 = logistic_regression(y_train_0, x_train_0, np.zeros(x_train_0.shape[1]), 10000, 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_1, loss_1 = logistic_regression(y_train_1, x_train_1, np.zeros(x_train_1.shape[1]), 10000, 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_2, loss_2 = logistic_regression(y_train_2, x_train_2, np.zeros(x_train_2.shape[1]), 10000, 1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. TEST ON TEST SET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_0 = zero_to_neg(np.around(sigmoid(x_test_0 @ w_0)))\n",
    "y_1 = zero_to_neg(np.around(sigmoid(x_test_1 @ w_1)))\n",
    "y_2 = zero_to_neg(np.around(sigmoid(x_test_2 @ w_2)))\n",
    "\n",
    "s_0 = np.column_stack((id_test_0, y_0))\n",
    "s_1 = np.column_stack((id_test_1, y_1))\n",
    "s_2 = np.column_stack((id_test_2, y_2))\n",
    "s = np.vstack((np.vstack((s_0, s_1)), s_2))\n",
    "s = sorted(s, key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s_df = pd.DataFrame(s)\n",
    "s_df.columns = ['Id', 'Prediction']\n",
    "s_df.Id = s_df.Id.apply(lambda x: int(x))\n",
    "s_df.Prediction = s_df.Prediction.apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s_df.to_csv('Data/12_submit_prediction_ridge_regression.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
