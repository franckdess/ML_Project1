{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from proj1_helpers import load_csv_data\n",
    "from functions import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%qtconsole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.   SETUP THE DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set = load_csv_data('Data/train.csv', sub_sample=False)\n",
    "test_set = load_csv_data('Data/test.csv', sub_sample=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate the data depending on the columns used which can be found by looking at the parameter 'PRI_jet_num'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The following separete the initial test set x_test in three subsets\n",
    "# according the feature PRI_jet_num which takes its value in the \n",
    "# set {0,1,2,3}\n",
    "\n",
    "x_test = test_set[1]\n",
    "y_test = neg_to_zero(test_set[0])\n",
    "ids_test = test_set[2]\n",
    "\n",
    "y_test_reshaped = np.reshape(y_test,(len(y_test),1))\n",
    "ids_test_reshaped = np.reshape(ids_test,(len(ids_test),1))\n",
    "\n",
    "\n",
    "# Concatenating x, y and ids: \n",
    "\n",
    "complete_test = np.concatenate((y_test_reshaped,x_test),axis=1)\n",
    "complete_test = np.concatenate((complete_test,ids_test_reshaped),axis=1)\n",
    "\n",
    "# Creating three subsets by the value of the feature PRI_jet_num \n",
    "# which is column 23\n",
    "\n",
    "\n",
    "# We create 3 sub arrays (Subset-0, Subset-1, Subset-23) \n",
    "# according the value of Pri_jet_num which is feature 23\n",
    "\n",
    "Subset_test_0 = complete_test[complete_test[:,23]==0]\n",
    "Subset_test_1 = complete_test[complete_test[:,23]==1]\n",
    "Subset_test_23 = complete_test[2<=complete_test[:,23]]\n",
    "\n",
    "# Separate the subsets by ids_test, y_test and x_test\n",
    "\n",
    "y_test_0 = Subset_test_0[:,0]\n",
    "y_test_1 = Subset_test_1[:,0]\n",
    "y_test_2 = Subset_test_23[:,0]\n",
    "\n",
    "x_test_0 = Subset_test_0[:,1:-1]\n",
    "x_test_1 = Subset_test_1[:,1:-1]\n",
    "x_test_2 = Subset_test_23[:,1:-1]\n",
    "\n",
    "id_test_0 = Subset_test_0[:,-1]\n",
    "id_test_1 = Subset_test_1[:,-1]\n",
    "id_test_2 = Subset_test_23[:,-1]\n",
    "\n",
    "\n",
    "\n",
    "# The following separete the initial training set x in three subsets\n",
    "# according the feature PRI_jet_num which takes its value in the \n",
    "# set {0,1,2,3}\n",
    "\n",
    "x = train_set[1]\n",
    "y = neg_to_zero(train_set[0])\n",
    "ids = train_set[2]\n",
    "y_reshaped=np.reshape(y,(len(y),1))\n",
    "ids_reshaped = np.reshape(ids,(len(ids),1))\n",
    "\n",
    "# Concatenating x, y and ids: \n",
    "\n",
    "complete = np.concatenate((y_reshaped,x),axis=1)\n",
    "complete = np.concatenate((complete,ids_reshaped),axis=1)\n",
    "\n",
    "# Creating three subsets by the value of the feature PRI_jet_num \n",
    "# which is column 23\n",
    "\n",
    "\n",
    "# We create 3 sub arrays (Subset-0, Subset-1, Subset-23) \n",
    "# according the value of Pri_jet_num which is feature 23\n",
    "\n",
    "Subset_0 = complete[complete[:,23]==0]\n",
    "Subset_1 = complete[complete[:,23]==1]\n",
    "Subset_23 = complete[2<=complete[:,23]]\n",
    "\n",
    "# Separate the subsets by ids, y and x\n",
    "\n",
    "y_train_0 = Subset_0[:,0]\n",
    "y_train_1 = Subset_1[:,0]\n",
    "y_train_2 = Subset_23[:,0]\n",
    "\n",
    "x_train_0 = Subset_0[:,1:-1]\n",
    "x_train_1 = Subset_1[:,1:-1]\n",
    "x_train_2 = Subset_23[:,1:-1]\n",
    "\n",
    "id_train_0 = Subset_0[:,-1]\n",
    "id_train_1 = Subset_1[:,-1]\n",
    "id_train_2 = Subset_23[:,-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We replace the -999 by the mean of the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120.66765385553533\n",
      "122.1821093424021\n",
      "122.81618442610974\n"
     ]
    }
   ],
   "source": [
    "# Computing the mean of feature DER_mass_MMC \n",
    "\n",
    "mean_0_train = get_column_mean_array(-999, x_0, 0)\n",
    "mean_1_train = get_column_mean_array(-999, x_1, 0)\n",
    "mean_23_train = get_column_mean_array(-999, x_23, 0)\n",
    "\n",
    "\n",
    "# Computing the mean of feature DER_mass_MMC for the test set\n",
    "\n",
    "mean_0_test = get_column_mean_array(-999, x_0, 0)\n",
    "mean_1_test = get_column_mean_array(-999, x_1, 0)\n",
    "mean_23_test = get_column_mean_array(-999, x_23, 0)\n",
    "\n",
    "# Replace in the train set the missing values by the means. \n",
    "for idx, value in enumerate(x_0[:,0]):\n",
    "    if x_0[idx,0]==-999:\n",
    "        x_0[idx,0]=mean_0_train\n",
    "    \n",
    "for idx, value in enumerate(x_1[:,0]):\n",
    "    if x_1[idx,0]==-999:\n",
    "        x_1[idx,0]=mean_1_train\n",
    "        \n",
    "for idx, value in enumerate(x_23[:,0]):\n",
    "    if x_23[idx,0]==-999:\n",
    "        x_23[idx,0]=mean_23_train\n",
    "        \n",
    "\n",
    "# Replace in the test set the missing values by the means \n",
    "\n",
    "for idx, value in enumerate(x_test_0[:,0]):\n",
    "    if x_test_0[idx,0]==-999:\n",
    "        x_test_0[idx,0]=mean_0_test\n",
    "    \n",
    "for idx, value in enumerate(x_test_1[:,0]):\n",
    "    if x_test_1[idx,0]==-999:\n",
    "        x_test_1[idx,0]=mean_1_test\n",
    "        \n",
    "for idx, value in enumerate(x_test_23[:,0]):\n",
    "    if x_test_23[idx,0]==-999:\n",
    "        x_test_23[idx,0]=mean_23_test\n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We standardize all observations and add the intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Standardized version\n",
    "x_train_0 = (x_train_0.T - x_train_0.mean(1)).T\n",
    "y_0 = x_train_0.std(1)\n",
    "x_train_0 = (x_train_0.T/y_0).T\n",
    "x_train_0 = np.column_stack((np.ones(len(x_train_0)), x_train_0))\n",
    "\n",
    "x_train_1 = (x_train_1.T - x_train_1.mean(1)).T\n",
    "y_1 = x_train_1.std(1)\n",
    "x_train_1 = (x_train_1.T/y_1).T\n",
    "x_train_1 = np.column_stack((np.ones(len(x_train_1)), x_train_1))\n",
    "\n",
    "x_train_2 = (x_train_2.T - x_train_2.mean(1)).T\n",
    "y_2 = x_train_2.std(1)\n",
    "x_train_2 = (x_train_2.T/y_2).T\n",
    "x_train_2 = np.column_stack((np.ones(len(x_train_2)), x_train_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Standardized version\n",
    "x_test_0 = (x_test_0.T - x_test_0.mean(1)).T\n",
    "y_te_0 = x_test_0.std(1)\n",
    "x_test_0 = (x_test_0.T/y_te_0).T\n",
    "x_test_0 = np.column_stack((np.ones(len(x_test_0)), x_test_0))\n",
    "\n",
    "x_test_1 = (x_test_1.T - x_test_1.mean(1)).T\n",
    "y_te_1 = x_test_1.std(1)\n",
    "x_test_1 = (x_test_1.T/y_te_1).T\n",
    "x_test_1 = np.column_stack((np.ones(len(x_test_1)), x_test_1))\n",
    "\n",
    "x_test_2 = (x_test_2.T - x_test_2.mean(1)).T\n",
    "y_te_2 = x_test_2.std(1)\n",
    "x_test_2 = (x_test_2.T/y_te_2).T\n",
    "x_test_2 = np.column_stack((np.ones(len(x_test_2)), x_test_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We replace all \"-1\" prediction by \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train_0 = neg_to_zero(y_train_0)\n",
    "y_train_1 = neg_to_zero(y_train_1)\n",
    "y_train_2 = neg_to_zero(y_train_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. DEFINE ALL FUNCTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    ws = [initial_w]\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        gradient = compute_gradient_least_square(y, tx, w)\n",
    "        loss = compute_mse(y, tx, w)\n",
    "        w = w - gamma*gradient\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def least_squares_SGD(y, tx, initial_w, max_iters, gamma):\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):    \n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size):\n",
    "            gradient = compute_stoch_gradient(minibatch_y, minibatch_tx, w)\n",
    "            loss = compute_mse(minibatch_y, minibatch_tx, w)\n",
    "            new_w = w - gamma*gradient\n",
    "            w = new_w\n",
    "            ws.append(w)\n",
    "            losses.append(loss)\n",
    "            print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "            bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return ws[len(ws)-1], losss[len(losses)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    A = tx.T@tx\n",
    "    b = tx.T@y\n",
    "    w = np.linalg.solve(A, b)\n",
    "    loss = compute_mse(y, tx, w)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_):\n",
    "    X = tx.T@tx\n",
    "    N = X.shape[0]\n",
    "    A = (X + 2*N*lambda_*np.identity(N))\n",
    "    b = tx.T@y\n",
    "    w = np.linalg.solve(A, b)\n",
    "    loss = compute_mse(y, tx, w)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression(y, tx, initial_w, max_iters, gamma):\n",
    "    loss, w = gradient_descent_log_reg(y, tx, initial_w, max_iters, gamma)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma):\n",
    "    loss, w = reg_gradient_descent_log_reg(y, tx, lambda_, initial_w, max_iters, gamma)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**3. GET THE MODEL**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make some tests to find the correct gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def cross_validation_visualization(lambds, mse_tr, mse_te):\n",
    "    \"\"\"visualization the curves of mse_tr and mse_te.\"\"\"\n",
    "    plt.semilogx(lambds, mse_tr, marker=\".\", color='b', label='train error')\n",
    "    plt.semilogx(lambds, mse_te, marker=\".\", color='r', label='test error')\n",
    "    plt.xlabel(\"lambda\")\n",
    "    plt.ylabel(\"rmse\")\n",
    "    plt.title(\"cross validation\")\n",
    "    plt.legend(loc=2)\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validation(y, x, k_indices, k, lambda_):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    # get k'th subgroup in test, others in train: TODO\n",
    "    \n",
    "    test_indices = k_indices[k]\n",
    "    train_indices = np.concatenate((k_indices[:k], k_indices[k+1:])).flatten()\n",
    "    \n",
    "    x_train = x[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    x_test = x[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    \n",
    "    # ridge regression: TODO\n",
    "    w_train, loss = logistic_regression(y_train, x_train, np.zeros(x_train.shape[1]), 1000, lambda_)\n",
    "    \n",
    "    # ***************************************************\n",
    "    # calculate the loss for train and test data: TODO\n",
    "    loss_tr = compute_mse(y_train, x_train, w_train)\n",
    "    loss_te = compute_mse(y_test, x_test, w_train)\n",
    "\n",
    "    return loss_tr, loss_te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def cross_validation_demo():\n",
    "    seed = 1\n",
    "    degree = 7\n",
    "    k_fold = 4\n",
    "    lambdas = np.logspace(-5, 0, 30)\n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y_train_2, k_fold, seed)\n",
    "    # define lists to store the loss of training data and test data\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    # ***************************************************\n",
    "    for lambda_ in lambdas:\n",
    "        err_train = []\n",
    "        err_test = []\n",
    "        for k in range (k_fold):\n",
    "            loss_tr, loss_te = cross_validation(y_train_2, x_train_2, k_indices, k, lambda_) \n",
    "            err_train.append(np.sqrt(2*loss_tr))\n",
    "            err_test.append(np.sqrt(2*loss_te))\n",
    "        rmse_tr.append(sum(err_train)/k_fold)\n",
    "        rmse_te.append(sum(err_test)/k_fold)\n",
    "    # ***************************************************    \n",
    "    cross_validation_visualization(lambdas, rmse_tr, rmse_te)\n",
    "\n",
    "cross_validation_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the model and the log for the 3 different sets of observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_0, loss_0 = logistic_regression(y_train_0, x_train_0, np.zeros(x_train_0.shape[1]), 10000, 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_1, loss_1 = logistic_regression(y_train_1, x_train_1, np.zeros(x_train_1.shape[1]), 10000, 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_2, loss_2 = logistic_regression(y_train_2, x_train_2, np.zeros(x_train_2.shape[1]), 10000, 1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. TEST ON TEST SET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_0 = zero_to_neg(np.around(sigmoid(x_test_0 @ w_0)))\n",
    "y_1 = zero_to_neg(np.around(sigmoid(x_test_1 @ w_1)))\n",
    "y_2 = zero_to_neg(np.around(sigmoid(x_test_2 @ w_2)))\n",
    "\n",
    "s_0 = np.column_stack((id_test_0, y_0))\n",
    "s_1 = np.column_stack((id_test_1, y_1))\n",
    "s_2 = np.column_stack((id_test_2, y_2))\n",
    "s = np.vstack((np.vstack((s_0, s_1)), s_2))\n",
    "s = sorted(s, key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s_df = pd.DataFrame(s)\n",
    "s_df.columns = ['Id', 'Prediction']\n",
    "s_df.Id = s_df.Id.apply(lambda x: int(x))\n",
    "s_df.Prediction = s_df.Prediction.apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s_df.to_csv('Data/12_submit_prediction_ridge_regression.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
