{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from proj1_helpers import load_csv_data\n",
    "from functions import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.   SETUP THE DATA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create pandas DataFrames to clear the train data and the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set_pd = pd.read_csv('Data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_pd = pd.read_csv('Data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate the data depending on the columns used which can be found by looking at the parameter 'PRI_jet_num'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set_pd_0 = train_set_pd[train_set_pd.PRI_jet_num == 0].drop(['DER_deltaeta_jet_jet', 'DER_mass_jet_jet', 'DER_prodeta_jet_jet', 'DER_lep_eta_centrality', 'PRI_jet_leading_pt', 'PRI_jet_leading_eta', 'PRI_jet_leading_phi', 'PRI_jet_subleading_pt', 'PRI_jet_subleading_eta', 'PRI_jet_subleading_phi', 'PRI_jet_num'], axis = 1)\n",
    "train_set_pd_1 = train_set_pd[train_set_pd.PRI_jet_num == 1].drop(['DER_deltaeta_jet_jet', 'DER_mass_jet_jet', 'DER_prodeta_jet_jet', 'DER_lep_eta_centrality', 'PRI_jet_subleading_pt', 'PRI_jet_subleading_eta', 'PRI_jet_subleading_phi', 'PRI_jet_num'], axis = 1)\n",
    "train_set_pd_2 = train_set_pd[train_set_pd.PRI_jet_num.isin([2, 3])].drop(['PRI_jet_num'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_set_pd_0 = test_set_pd[test_set_pd.PRI_jet_num == 0].drop(['DER_deltaeta_jet_jet', 'DER_mass_jet_jet', 'DER_prodeta_jet_jet', 'DER_lep_eta_centrality', 'PRI_jet_leading_pt', 'PRI_jet_leading_eta', 'PRI_jet_leading_phi', 'PRI_jet_subleading_pt', 'PRI_jet_subleading_eta', 'PRI_jet_subleading_phi', 'PRI_jet_num'], axis = 1)\n",
    "test_set_pd_1 = test_set_pd[test_set_pd.PRI_jet_num == 1].drop(['DER_deltaeta_jet_jet', 'DER_mass_jet_jet', 'DER_prodeta_jet_jet', 'DER_lep_eta_centrality', 'PRI_jet_subleading_pt', 'PRI_jet_subleading_eta', 'PRI_jet_subleading_phi', 'PRI_jet_num'], axis = 1)\n",
    "test_set_pd_2 = test_set_pd[test_set_pd.PRI_jet_num.isin([2, 3])].drop(['PRI_jet_num'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We replace the -999 by the mean of the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_0_tr = get_column_mean(-999.000, train_set_pd_0, 'DER_mass_MMC')\n",
    "mean_1_tr = get_column_mean(-999.000, train_set_pd_1, 'DER_mass_MMC')\n",
    "mean_2_tr = get_column_mean(-999.000, train_set_pd_2, 'DER_mass_MMC')\n",
    "\n",
    "train_set_pd_0['DER_mass_MMC'] = train_set_pd_0['DER_mass_MMC'].apply(lambda x: replace_by_mean(x, mean_0_tr, 'DER_mass_MMC'))\n",
    "train_set_pd_1['DER_mass_MMC'] = train_set_pd_1['DER_mass_MMC'].apply(lambda x: replace_by_mean(x, mean_1_tr, 'DER_mass_MMC'))\n",
    "train_set_pd_2['DER_mass_MMC'] = train_set_pd_2['DER_mass_MMC'].apply(lambda x: replace_by_mean(x, mean_2_tr, 'DER_mass_MMC'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_0_te = get_column_mean(-999.000, test_set_pd_0, 'DER_mass_MMC')\n",
    "mean_1_te = get_column_mean(-999.000, test_set_pd_1, 'DER_mass_MMC')\n",
    "mean_2_te = get_column_mean(-999.000, test_set_pd_2, 'DER_mass_MMC')\n",
    "\n",
    "test_set_pd_0['DER_mass_MMC'] = test_set_pd_0['DER_mass_MMC'].apply(lambda x: replace_by_mean(x, mean_0_te, 'DER_mass_MMC'))\n",
    "test_set_pd_1['DER_mass_MMC'] = test_set_pd_1['DER_mass_MMC'].apply(lambda x: replace_by_mean(x, mean_1_te, 'DER_mass_MMC'))\n",
    "test_set_pd_2['DER_mass_MMC'] = test_set_pd_2['DER_mass_MMC'].apply(lambda x: replace_by_mean(x, mean_2_te, 'DER_mass_MMC'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the three DataFrames then we can open them with the load_csv function, which returns usable tuples of y's, x's and ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set_pd_0.to_csv('Data/train_set_0.csv', index = False)\n",
    "train_set_pd_1.to_csv('Data/train_set_1.csv', index = False)\n",
    "train_set_pd_2.to_csv('Data/train_set_2.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_set_pd_0.to_csv('Data/test_set_0.csv', index = False)\n",
    "test_set_pd_1.to_csv('Data/test_set_1.csv', index = False)\n",
    "test_set_pd_2.to_csv('Data/test_set_2.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set_0 = load_csv_data('Data/train_set_0.csv', sub_sample=True)\n",
    "train_set_1 = load_csv_data('Data/train_set_1.csv', sub_sample=True)\n",
    "train_set_2 = load_csv_data('Data/train_set_2.csv', sub_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_set_0 = load_csv_data('Data/test_set_0.csv', sub_sample=False)\n",
    "test_set_1 = load_csv_data('Data/test_set_1.csv', sub_sample=False)\n",
    "test_set_2 = load_csv_data('Data/test_set_2.csv', sub_sample=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We separate x's the y's and the id's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_0 = train_set_0[1]\n",
    "x_train_1 = train_set_1[1]\n",
    "x_train_2 = train_set_2[1]\n",
    "\n",
    "y_train_0 = train_set_0[0]\n",
    "y_train_1 = train_set_1[0]\n",
    "y_train_2 = train_set_2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_0 = test_set_0[1]\n",
    "x_test_1 = test_set_1[1]\n",
    "x_test_2 = test_set_2[1]\n",
    "\n",
    "id_test_0 = test_set_0[2]\n",
    "id_test_1 = test_set_1[2]\n",
    "id_test_2 = test_set_2[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We standardize all observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardized version\n",
    "x_train_0 = (x_train_0.T - x_train_0.mean(1)).T\n",
    "y_0 = x_train_0.std(1)\n",
    "x_train_0 = (x_train_0.T/y_0).T\n",
    "x_train_0 = np.column_stack((np.ones(len(x_train_0)), x_train_0))\n",
    "\n",
    "x_train_1 = (x_train_1.T - x_train_1.mean(1)).T\n",
    "y_1 = x_train_1.std(1)\n",
    "x_train_1 = (x_train_1.T/y_1).T\n",
    "x_train_1 = np.column_stack((np.ones(len(x_train_1)), x_train_1))\n",
    "\n",
    "x_train_2 = (x_train_2.T - x_train_2.mean(1)).T\n",
    "y_2 = x_train_2.std(1)\n",
    "x_train_2 = (x_train_2.T/y_2).T\n",
    "x_train_2 = np.column_stack((np.ones(len(x_train_2)), x_train_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardized version\n",
    "x_test_0 = (x_test_0.T - x_test_0.mean(1)).T\n",
    "y_te_0 = x_test_0.std(1)\n",
    "x_test_0 = (x_test_0.T/y_te_0).T\n",
    "x_test_0 = np.column_stack((np.ones(len(x_test_0)), x_test_0))\n",
    "\n",
    "x_test_1 = (x_test_1.T - x_test_1.mean(1)).T\n",
    "y_te_1 = x_test_1.std(1)\n",
    "x_test_1 = (x_test_1.T/y_te_1).T\n",
    "x_test_1 = np.column_stack((np.ones(len(x_test_1)), x_test_1))\n",
    "\n",
    "x_test_2 = (x_test_2.T - x_test_2.mean(1)).T\n",
    "y_te_2 = x_test_2.std(1)\n",
    "x_test_2 = (x_test_2.T/y_te_2).T\n",
    "x_test_2 = np.column_stack((np.ones(len(x_test_2)), x_test_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We replace all \"-1\" prediction by \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train_0 = neg_to_zero(y_train_0)\n",
    "y_train_1 = neg_to_zero(y_train_1)\n",
    "y_train_2 = neg_to_zero(y_train_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. DEFINE ALL FUNCTION **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    ws = [initial_w]\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        gradient = compute_gradient_least_square(y, tx, w)\n",
    "        loss = compute_mse(y, tx, w)\n",
    "        w = w - gamma*gradient\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def least_squares_SGD(y, tx, initial_w, max_iters, gamma):\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):    \n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size):\n",
    "            gradient = compute_stoch_gradient(minibatch_y, minibatch_tx, w)\n",
    "            loss = compute_mse(minibatch_y, minibatch_tx, w)\n",
    "            new_w = w - gamma*gradient\n",
    "            w = new_w\n",
    "            ws.append(w)\n",
    "            losses.append(loss)\n",
    "            print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "            bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    A = tx.T@tx\n",
    "    b = tx.T@y\n",
    "    w = np.linalg.solve(A, b)\n",
    "    loss = compute_mse(y, tx, w)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_):\n",
    "    X = tx.T@tx\n",
    "    N = X.shape[0]\n",
    "    A = (X + 2*N*lambda_*np.identity(N))\n",
    "    b = tx.T@y\n",
    "    w = np.linalg.solve(A, b)\n",
    "    loss = compute_mse(y, tx, w)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression(y, tx, initial_w, max_iters, gamma):\n",
    "    loss, w = stoch_gradient_descent_log_reg(y, tx, initial_w, max_iters, gamma)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma):\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. GET THE MODEL**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make some tests to find the correct gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gammas = np.logspace(-5, 0)\n",
    "losses_0 = []\n",
    "losses_1 = []\n",
    "losses_2 = []\n",
    "for gamma in gammas:\n",
    "    w_0, loss_0 = logistic_regression(y_train_0, x_train_0, np.zeros(x_train_0.shape[1]), 1000, gamma)\n",
    "    w_1, loss_1 = logistic_regression(y_train_1, x_train_1, np.zeros(x_train_1.shape[1]), 1000, gamma)\n",
    "    w_2, loss_2 = logistic_regression(y_train_2, x_train_2, np.zeros(x_train_2.shape[1]), 1000, gamma)\n",
    "    \n",
    "    losses_0.append((gamma, loss_0))\n",
    "    losses_1.append((gamma, loss_1))\n",
    "    losses_2.append((gamma, loss_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_0.sort(key = lambda x: x[1])\n",
    "l_0  = np.take(losses_0, [x for x in range(20)], axis = 0)\n",
    "gamma_0 = l_0.mean(axis = 0)[0]\n",
    "\n",
    "losses_1.sort(key = lambda x: x[1])\n",
    "l_1  = np.take(losses_1, [x for x in range(20)], axis = 0)\n",
    "gamma_1 = l_1.mean(axis = 0)[0]\n",
    "\n",
    "losses_2.sort(key = lambda x: x[1])\n",
    "l_2  = np.take(losses_2, [x for x in range(20)], axis = 0)\n",
    "gamma_2 = l_2.mean(axis = 0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.039085977227429299, 0.016980548002622053, 0.021153959536203651)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma_0, gamma_1, gamma_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We commpute the model and the log for the 3 different sets of observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_0, loss_0 = logistic_regression(y_train_0, x_train_0, np.zeros(x_train_0.shape[1]), 10000, gamma_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_1, loss_1 = logistic_regression(y_train_1, x_train_1, np.zeros(x_train_1.shape[1]), 10000, gamma_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_2, loss_2 = logistic_regression(y_train_2, x_train_2, np.zeros(x_train_2.shape[1]), 10000, gamma_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. TEST ON TEST SET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_0 = zero_to_neg(np.around(sigmoid(x_test_0 @ w_0)))\n",
    "y_1 = zero_to_neg(np.around(sigmoid(x_test_1 @ w_1)))\n",
    "y_2 = zero_to_neg(np.around(sigmoid(x_test_2 @ w_2)))\n",
    "\n",
    "s_0 = np.column_stack((id_test_0, y_0))\n",
    "s_1 = np.column_stack((id_test_1, y_1))\n",
    "s_2 = np.column_stack((id_test_2, y_2))\n",
    "s = np.vstack((np.vstack((s_0, s_1)), s_2))\n",
    "s = sorted(s, key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s_df = pd.DataFrame(s)\n",
    "s_df.columns = ['Id', 'Prediction']\n",
    "s_df.Id = s_df.Id.apply(lambda x: int(x))\n",
    "s_df.Prediction = s_df.Prediction.apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_df.to_csv('Data/4_submit_prediction_st_ones_pr.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
