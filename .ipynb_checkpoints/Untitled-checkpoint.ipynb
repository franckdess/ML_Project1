{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from proj1_helpers import load_csv_data\n",
    "from functions import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. LOAD THE DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = load_csv_data('Data/train.csv', sub_sample=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. SET UP THE DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_set[1]\n",
    "y = neg_to_zero(train_set[0])\n",
    "ids = train_set[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "separator = int(len(x)*10/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test = zero_to_neg(y[:separator])\n",
    "x_test = x[:separator, :]\n",
    "ids_test = ids[:separator]\n",
    "control = np.column_stack((ids_test, y_test)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = y[separator+1:]\n",
    "x_train = x[separator+1:, :]\n",
    "ids_train = ids[separator+1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. CLEAN THE DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "na_indices = get_na_columns(x_train, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 5, 6, 12, 26, 27, 28]"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "na_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_clean = np.delete(x_train, na_indices, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test_clean = np.delete(x_test, na_indices, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "na_indices_rem_train = get_na_columns(x_train_clean, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 19, 20, 21]"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "na_indices_rem_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_0_train, y_0_train, x_0_test, indices_to_drop_0, indices_to_keep_0 = split_data(x_train_clean, 0, na_indices_rem_train)\n",
    "x_19_train, y_19_train, x_19_test, indices_to_drop_19, indices_to_keep_19 = split_data(x_train_clean, 19, na_indices_rem_train)\n",
    "x_20_train, y_20_train, x_20_test, indices_to_drop_20, indices_to_keep_20 = split_data(x_train_clean, 20, na_indices_rem_train)\n",
    "x_21_train, y_21_train, x_21_test, indices_to_drop_21, indices_to_keep_21 = split_data(x_train_clean, 21, na_indices_rem_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_0, loss_0 = ridge_regression(y_0_train, build_poly(x_0_train, degree = 3), lambda_ = 0.03)\n",
    "w_19, loss_19 = ridge_regression(y_19_train, build_poly(x_19_train, degree = 3), lambda_ = 0.03)\n",
    "w_20, loss_20 = ridge_regression(y_20_train, build_poly(x_20_train, degree = 3), lambda_ = 0.03)\n",
    "w_21, loss_21 = ridge_regression(y_21_train, build_poly(x_21_train, degree = 3), lambda_ = 0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_0_test = build_poly(x_0_test, degree = 3)@w_0\n",
    "y_19_test = build_poly(x_19_test, degree = 3)@w_19\n",
    "y_20_test = build_poly(x_20_test, degree = 3)@w_20\n",
    "y_21_test = build_poly(x_21_test, degree = 3)@w_21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_0_arr = np.column_stack((indices_to_drop_0, y_0_test))\n",
    "y_19_arr = np.column_stack((indices_to_drop_19, y_19_test))\n",
    "y_20_arr = np.column_stack((indices_to_drop_20, y_20_test))\n",
    "y_21_arr = np.column_stack((indices_to_drop_21, y_21_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_clean = put_back_y(x_train_clean, 0, y_0_arr)\n",
    "x_train_clean = put_back_y(x_train_clean, 19, y_19_arr)\n",
    "x_train_clean = put_back_y(x_train_clean, 20, y_20_arr)\n",
    "x_train_clean = put_back_y(x_train_clean, 21, y_21_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "na_indices_rem_test = get_na_columns(x_test_clean, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 19, 20, 21]"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "na_indices_rem_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_0_test_train, y_0_test_train, x_0_test_test, indices_to_drop_0_test, indices_to_keep_0_test = split_data(x_test_clean, 0, na_indices_rem_test)\n",
    "x_19_test_train, y_19_test_train, x_19_test_test, indices_to_drop_19_test, indices_to_keep_19_test = split_data(x_test_clean, 19, na_indices_rem_test)\n",
    "x_20_test_train, y_20_test_train, x_20_test_test, indices_to_drop_20_test, indices_to_keep_20_test = split_data(x_test_clean, 20, na_indices_rem_test)\n",
    "x_21_test_train, y_21_test_train, x_21_test_test, indices_to_drop_21_test, indices_to_keep_21_test = split_data(x_test_clean, 21, na_indices_rem_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_0_test_test = build_poly(x_0_test_test, degree = 3)@w_0\n",
    "y_19_test_test = build_poly(x_19_test_test, degree = 3)@w_19\n",
    "y_20_test_test = build_poly(x_20_test_test, degree = 3)@w_20\n",
    "y_21_test_test = build_poly(x_21_test_test, degree = 3)@w_21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_0_arr_test = np.column_stack((indices_to_drop_0_test, y_0_test_test))\n",
    "y_19_arr_test = np.column_stack((indices_to_drop_19_test, y_19_test_test))\n",
    "y_20_arr_test = np.column_stack((indices_to_drop_20_test, y_20_test_test))\n",
    "y_21_arr_test = np.column_stack((indices_to_drop_21_test, y_21_test_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_clean = put_back_y(x_test_clean, 0, y_0_arr_test)\n",
    "x_test_clean = put_back_y(x_test_clean, 19, y_19_arr_test)\n",
    "x_test_clean = put_back_y(x_test_clean, 20, y_20_arr_test)\n",
    "x_test_clean = put_back_y(x_test_clean, 21, y_21_arr_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. DEFINE THE FUNCTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    A = tx.T@tx\n",
    "    b = tx.T@y\n",
    "    w = np.linalg.solve(A, b)\n",
    "    loss = compute_mse(y, tx, w)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    ws = [initial_w]\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        gradient = compute_gradient_least_square(y, tx, w)\n",
    "        loss = compute_mse(y, tx, w)\n",
    "        w = w - gamma*gradient\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def least_squares_SGD(y, tx, initial_w, max_iters, gamma):\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):    \n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size):\n",
    "            gradient = compute_stoch_gradient(minibatch_y, minibatch_tx, w)\n",
    "            loss = compute_mse(minibatch_y, minibatch_tx, w)\n",
    "            new_w = w - gamma*gradient\n",
    "            w = new_w\n",
    "            ws.append(w)\n",
    "            losses.append(loss)\n",
    "            print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "            bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return ws, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_):\n",
    "    X = tx.T@tx\n",
    "    N = X.shape[0]\n",
    "    A = (X + 2*N*lambda_*np.identity(N))\n",
    "    b = tx.T@y\n",
    "    w = np.linalg.solve(A, b)\n",
    "    loss = compute_mse(y, tx, w)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression(y, tx, initial_w, max_iters, gamma):\n",
    "    loss, w = gradient_descent_log_reg(y, tx, initial_w, max_iters, gamma)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma):\n",
    "    loss, w = reg_gradient_descent_log_reg(y, tx, lambda_, initial_w, max_iters, gamma)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.CROSS VALIDATION VIZUALIZATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validation_visualization(lambds, mse_tr, mse_te):\n",
    "    \"\"\"visualization the curves of mse_tr and mse_te.\"\"\"\n",
    "    plt.semilogx(lambds, mse_tr, marker=\".\", color='b', label='train error')\n",
    "    plt.semilogx(lambds, mse_te, marker=\".\", color='r', label='test error')\n",
    "    plt.xlabel(\"lambda\")\n",
    "    plt.ylabel(\"rmse\")\n",
    "    plt.title(\"cross validation\")\n",
    "    plt.legend(loc=2)\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"cross_validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_poly(x, degree):\n",
    "    poly = x\n",
    "    for deg in range(2, degree+1):\n",
    "        poly = np.concatenate((poly, np.power(x, deg)), axis = 1)\n",
    "    return poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validation(y, x, k_indices, k, lambda_, degree):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    # ***************************************************\n",
    "    # get k'th subgroup in test, others in train: TODO\n",
    "    \n",
    "    test_indices = k_indices[k]\n",
    "    train_indices = np.concatenate((k_indices[:k], k_indices[k+1:])).flatten()\n",
    "    \n",
    "    x_train = x[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    x_test = x[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    # ***************************************************\n",
    "    # form data with polynomial degree: TODO\n",
    "    m_train = build_poly(x_train, degree)\n",
    "    m_test = build_poly(x_test, degree)\n",
    "    \n",
    "    # ***************************************************\n",
    "    # ridge regression: TODO\n",
    "    w_train, loss = ridge_regression(y_train, m_train, lambda_)\n",
    "    \n",
    "    # ***************************************************\n",
    "    # calculate the loss for train and test data: TODO\n",
    "    loss_tr = compute_mse(y_train, m_train, w_train)\n",
    "    loss_te = compute_mse(y_test, m_test, w_train)\n",
    "\n",
    "    #loss_tr = compute_loss_log_reg(sigmoid(m_train@w_train), y_train)\n",
    "    #loss_te = compute_loss_log_reg(sigmoid(m_test@w_train), y_test)\n",
    "    return loss_tr, loss_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_6 = build_poly(x_train_clean, degree = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224999, 138)"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validation_demo(y, x, degree):\n",
    "    seed = 1\n",
    "    k_fold = 4\n",
    "    lambdas = np.logspace(-9, 0, 30)\n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    # define lists to store the loss of training data and test data\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    # ***************************************************\n",
    "    for lambda_ in lambdas:\n",
    "        err_train = []\n",
    "        err_test = []\n",
    "        for k in range (k_fold):\n",
    "            loss_tr, loss_te = cross_validation(y, x, k_indices, k, lambda_, degree) \n",
    "            err_train.append(np.sqrt(2*loss_tr))\n",
    "            err_test.append(np.sqrt(2*loss_te))\n",
    "        rmse_tr.append(sum(err_train)/k_fold)\n",
    "        rmse_te.append(sum(err_test)/k_fold)\n",
    "    # ***************************************************    \n",
    "    cross_validation_visualization(lambdas, rmse_tr, rmse_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEaCAYAAAACBmAUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmz4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x133a02ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cross_validation_demo(y_train, x_train, degree = 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. GET THE MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/999): loss=155952.9225438064\n",
      "Gradient Descent(1/999): loss=888312.8246235647\n",
      "Gradient Descent(2/999): loss=900113.583475108\n",
      "Gradient Descent(3/999): loss=1529364.584245788\n",
      "Gradient Descent(4/999): loss=888312.8246235647\n",
      "Gradient Descent(5/999): loss=888312.8246235647\n",
      "Gradient Descent(6/999): loss=888312.8246235647\n",
      "Gradient Descent(7/999): loss=1523941.9916418106\n",
      "Gradient Descent(8/999): loss=888312.8246235647\n",
      "Gradient Descent(9/999): loss=888312.8246235647\n",
      "Gradient Descent(10/999): loss=888312.8246235647\n",
      "Gradient Descent(11/999): loss=1518127.9592320262\n",
      "Gradient Descent(12/999): loss=888312.8246235647\n",
      "Gradient Descent(13/999): loss=888312.8246235647\n",
      "Gradient Descent(14/999): loss=888589.135074723\n",
      "Gradient Descent(15/999): loss=1513234.9616594354\n",
      "Gradient Descent(16/999): loss=888312.8246235647\n",
      "Gradient Descent(17/999): loss=888312.8246235647\n",
      "Gradient Descent(18/999): loss=896348.8535780793\n",
      "Gradient Descent(19/999): loss=1507006.4635729126\n",
      "Gradient Descent(20/999): loss=888312.8246235647\n",
      "Gradient Descent(21/999): loss=888312.8246235647\n",
      "Gradient Descent(22/999): loss=911638.0318754932\n",
      "Gradient Descent(23/999): loss=1513764.556690821\n",
      "Gradient Descent(24/999): loss=888312.8246235647\n",
      "Gradient Descent(25/999): loss=888312.8246235647\n",
      "Gradient Descent(26/999): loss=918327.047380612\n",
      "Gradient Descent(27/999): loss=1508226.8347321947\n",
      "Gradient Descent(28/999): loss=888312.8246235647\n",
      "Gradient Descent(29/999): loss=890580.872910154\n",
      "Gradient Descent(30/999): loss=935630.9893843872\n",
      "Gradient Descent(31/999): loss=947155.4377847721\n",
      "Gradient Descent(32/999): loss=917958.6334457345\n",
      "Gradient Descent(33/999): loss=1513568.8367879177\n",
      "Gradient Descent(34/999): loss=888312.8246235647\n",
      "Gradient Descent(35/999): loss=888358.8763654245\n",
      "Gradient Descent(36/999): loss=1031199.8666786898\n",
      "Gradient Descent(37/999): loss=907608.5044627711\n",
      "Gradient Descent(38/999): loss=1510034.3656001869\n",
      "Gradient Descent(39/999): loss=888312.8246235647\n",
      "Gradient Descent(40/999): loss=895232.0988379817\n",
      "Gradient Descent(41/999): loss=1064311.0690758007\n",
      "Gradient Descent(42/999): loss=907735.1467528854\n",
      "Gradient Descent(43/999): loss=1487756.8354755666\n",
      "Gradient Descent(44/999): loss=888312.8246235647\n",
      "Gradient Descent(45/999): loss=910187.4020069134\n",
      "Gradient Descent(46/999): loss=1469716.0656020364\n",
      "Gradient Descent(47/999): loss=888312.8246235647\n",
      "Gradient Descent(48/999): loss=911994.9328749059\n",
      "Gradient Descent(49/999): loss=1452043.7096633841\n",
      "Gradient Descent(50/999): loss=889360.5017508724\n",
      "Gradient Descent(51/999): loss=913503.1274208103\n",
      "Gradient Descent(52/999): loss=1424378.1257411807\n",
      "Gradient Descent(53/999): loss=890788.1057485228\n",
      "Gradient Descent(54/999): loss=915506.3781917066\n",
      "Gradient Descent(55/999): loss=1438723.2433304715\n",
      "Gradient Descent(56/999): loss=890477.2564909699\n",
      "Gradient Descent(57/999): loss=913710.360259179\n",
      "Gradient Descent(58/999): loss=1405646.579739755\n",
      "Gradient Descent(59/999): loss=892722.2789066292\n",
      "Gradient Descent(60/999): loss=915748.1498364697\n",
      "Gradient Descent(61/999): loss=1430583.5979567727\n",
      "Gradient Descent(62/999): loss=891789.731133971\n",
      "Gradient Descent(63/999): loss=913215.3040341872\n",
      "Gradient Descent(64/999): loss=1394294.825371344\n",
      "Gradient Descent(65/999): loss=893401.5420990594\n",
      "Gradient Descent(66/999): loss=914711.9856446269\n",
      "Gradient Descent(67/999): loss=1412116.8494710405\n",
      "Gradient Descent(68/999): loss=893113.7187124366\n",
      "Gradient Descent(69/999): loss=913848.5154847578\n",
      "Gradient Descent(70/999): loss=1410240.2409902583\n",
      "Gradient Descent(71/999): loss=894460.7321618319\n",
      "Gradient Descent(72/999): loss=917348.4478660938\n",
      "Gradient Descent(73/999): loss=1426024.4755126643\n",
      "Gradient Descent(74/999): loss=893631.8008083581\n",
      "Gradient Descent(75/999): loss=915195.5289341534\n",
      "Gradient Descent(76/999): loss=1417919.3689453606\n",
      "Gradient Descent(77/999): loss=893862.0595176565\n",
      "Gradient Descent(78/999): loss=915391.2488370573\n",
      "Gradient Descent(79/999): loss=1417366.7480430445\n",
      "Gradient Descent(80/999): loss=895658.077450184\n",
      "Gradient Descent(81/999): loss=918062.2498649188\n",
      "Gradient Descent(82/999): loss=1434808.845272398\n",
      "Gradient Descent(83/999): loss=894115.3440978847\n",
      "Gradient Descent(84/999): loss=916646.1588027333\n",
      "Gradient Descent(85/999): loss=1427935.6227998405\n",
      "Gradient Descent(86/999): loss=895220.5859025168\n",
      "Gradient Descent(87/999): loss=915909.3309329785\n",
      "Gradient Descent(88/999): loss=1424170.892902812\n",
      "Gradient Descent(89/999): loss=895773.2068048331\n",
      "Gradient Descent(90/999): loss=915748.1498364699\n",
      "Gradient Descent(91/999): loss=1415916.1181744642\n",
      "Gradient Descent(92/999): loss=899215.5745088443\n",
      "Gradient Descent(93/999): loss=922379.6006642638\n",
      "Gradient Descent(94/999): loss=1415685.859465166\n",
      "Gradient Descent(95/999): loss=907712.1208819555\n",
      "Gradient Descent(96/999): loss=1040836.193662828\n",
      "Gradient Descent(97/999): loss=912443.9373580378\n",
      "Gradient Descent(98/999): loss=1441636.0160030958\n",
      "Gradient Descent(99/999): loss=897546.1988664311\n",
      "Gradient Descent(100/999): loss=921873.0315038072\n",
      "Gradient Descent(101/999): loss=1434060.5044671786\n",
      "Gradient Descent(102/999): loss=903199.0501797068\n",
      "Gradient Descent(103/999): loss=957045.0493491386\n",
      "Gradient Descent(104/999): loss=915080.3995795044\n",
      "Gradient Descent(105/999): loss=1453206.5161453409\n",
      "Gradient Descent(106/999): loss=889982.2002659782\n",
      "Gradient Descent(107/999): loss=912570.5796481519\n",
      "Gradient Descent(108/999): loss=1444272.4782245627\n",
      "Gradient Descent(109/999): loss=891386.7783926985\n",
      "Gradient Descent(110/999): loss=913364.9721952312\n",
      "Gradient Descent(111/999): loss=1437122.945300848\n",
      "Gradient Descent(112/999): loss=893297.9256798754\n",
      "Gradient Descent(113/999): loss=916220.1801905313\n",
      "Gradient Descent(114/999): loss=1445043.8449007126\n",
      "Gradient Descent(115/999): loss=893182.7963252261\n",
      "Gradient Descent(116/999): loss=915126.4513213641\n",
      "Gradient Descent(117/999): loss=1438953.5020397701\n",
      "Gradient Descent(118/999): loss=893850.5465821915\n",
      "Gradient Descent(119/999): loss=915034.3478376447\n",
      "Gradient Descent(120/999): loss=1428384.6272829727\n",
      "Gradient Descent(121/999): loss=898144.8715106068\n",
      "Gradient Descent(122/999): loss=921631.2598590441\n",
      "Gradient Descent(123/999): loss=1422697.237163302\n",
      "Gradient Descent(124/999): loss=907884.8149139293\n",
      "Gradient Descent(125/999): loss=1337858.4157223052\n",
      "Gradient Descent(126/999): loss=910406.1477807469\n",
      "Gradient Descent(127/999): loss=1406256.765319396\n",
      "Gradient Descent(128/999): loss=910383.1219098172\n",
      "Gradient Descent(129/999): loss=1126883.8733276417\n",
      "Gradient Descent(130/999): loss=914090.2871295214\n",
      "Gradient Descent(131/999): loss=1435096.6686590216\n",
      "Gradient Descent(132/999): loss=908840.3885575176\n",
      "Gradient Descent(133/999): loss=1021045.4575986302\n",
      "Gradient Descent(134/999): loss=915644.5334172854\n",
      "Gradient Descent(135/999): loss=1454069.98630521\n",
      "Gradient Descent(136/999): loss=896820.8839321407\n",
      "Gradient Descent(137/999): loss=922817.0922119308\n",
      "Gradient Descent(138/999): loss=1452239.4295662874\n",
      "Gradient Descent(139/999): loss=893033.1281641822\n",
      "Gradient Descent(140/999): loss=916312.2836742509\n",
      "Gradient Descent(141/999): loss=1448025.6951861267\n",
      "Gradient Descent(142/999): loss=892952.5376159278\n",
      "Gradient Descent(143/999): loss=915529.4040626363\n",
      "Gradient Descent(144/999): loss=1442441.9214856406\n",
      "Gradient Descent(145/999): loss=893378.5162281299\n",
      "Gradient Descent(146/999): loss=914573.8304190479\n",
      "Gradient Descent(147/999): loss=1434739.7676596087\n",
      "Gradient Descent(148/999): loss=895186.047096122\n",
      "Gradient Descent(149/999): loss=917947.1205102694\n",
      "Gradient Descent(150/999): loss=1440265.9766827705\n",
      "Gradient Descent(151/999): loss=898732.0312193177\n",
      "Gradient Descent(152/999): loss=924820.342982827\n",
      "Gradient Descent(153/999): loss=1436662.4278822509\n",
      "Gradient Descent(154/999): loss=908495.00049357\n",
      "Gradient Descent(155/999): loss=1132536.7246409177\n",
      "Gradient Descent(156/999): loss=912524.5279062919\n",
      "Gradient Descent(157/999): loss=1441163.9856490344\n",
      "Gradient Descent(158/999): loss=908230.2029778768\n",
      "Gradient Descent(159/999): loss=1033652.1219327181\n",
      "Gradient Descent(160/999): loss=913744.8990655736\n",
      "Gradient Descent(161/999): loss=1455244.3057226313\n",
      "Gradient Descent(162/999): loss=893885.0853885863\n",
      "Gradient Descent(163/999): loss=917866.5299620151\n",
      "Gradient Descent(164/999): loss=1453367.6972418497\n",
      "Gradient Descent(165/999): loss=893643.3137438229\n",
      "Gradient Descent(166/999): loss=916231.6931259966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(167/999): loss=1442994.5423879565\n",
      "Gradient Descent(168/999): loss=895335.7152571662\n",
      "Gradient Descent(169/999): loss=919743.138442797\n",
      "Gradient Descent(170/999): loss=1449844.7389895841\n",
      "Gradient Descent(171/999): loss=897638.3023501502\n",
      "Gradient Descent(172/999): loss=921884.5444392725\n",
      "Gradient Descent(173/999): loss=1431711.865632335\n",
      "Gradient Descent(174/999): loss=907712.1208819555\n",
      "Gradient Descent(175/999): loss=1071667.8348378844\n",
      "Gradient Descent(176/999): loss=912294.2691969937\n",
      "Gradient Descent(177/999): loss=1444629.3792239756\n",
      "Gradient Descent(178/999): loss=905881.5641430335\n",
      "Gradient Descent(179/999): loss=986748.4228486328\n",
      "Gradient Descent(180/999): loss=914516.2657417232\n",
      "Gradient Descent(181/999): loss=1461426.7520672937\n",
      "Gradient Descent(182/999): loss=891317.700779909\n",
      "Gradient Descent(183/999): loss=915471.8393853116\n",
      "Gradient Descent(184/999): loss=1453885.7793377712\n",
      "Gradient Descent(185/999): loss=892215.709746173\n",
      "Gradient Descent(186/999): loss=914838.6279347412\n",
      "Gradient Descent(187/999): loss=1444214.9135472379\n",
      "Gradient Descent(188/999): loss=894288.0381298584\n",
      "Gradient Descent(189/999): loss=917440.5513498132\n",
      "Gradient Descent(190/999): loss=1445561.9269966336\n",
      "Gradient Descent(191/999): loss=898398.1560908351\n",
      "Gradient Descent(192/999): loss=925338.4250787481\n",
      "Gradient Descent(193/999): loss=1441854.7617769297\n",
      "Gradient Descent(194/999): loss=908587.1039772893\n",
      "Gradient Descent(195/999): loss=1140319.4690152039\n",
      "Gradient Descent(196/999): loss=913111.687615003\n",
      "Gradient Descent(197/999): loss=1446897.4275105647\n",
      "Gradient Descent(198/999): loss=907999.9442685787\n",
      "Gradient Descent(199/999): loss=1035597.8080262895\n",
      "Gradient Descent(200/999): loss=913480.1015498803\n",
      "Gradient Descent(201/999): loss=1457696.5609766599\n",
      "Gradient Descent(202/999): loss=897039.6297059746\n",
      "Gradient Descent(203/999): loss=921861.5185683424\n",
      "Gradient Descent(204/999): loss=1442902.4389042372\n",
      "Gradient Descent(205/999): loss=905363.4820471118\n",
      "Gradient Descent(206/999): loss=978102.2083144778\n",
      "Gradient Descent(207/999): loss=916496.4906416895\n",
      "Gradient Descent(208/999): loss=1476577.775139129\n",
      "Gradient Descent(209/999): loss=889602.273395636\n",
      "Gradient Descent(210/999): loss=915057.3737085746\n",
      "Gradient Descent(211/999): loss=1462934.9466131981\n",
      "Gradient Descent(212/999): loss=891409.8042636287\n",
      "Gradient Descent(213/999): loss=915379.7359015922\n",
      "Gradient Descent(214/999): loss=1452642.3823075597\n",
      "Gradient Descent(215/999): loss=892906.4858740679\n",
      "Gradient Descent(216/999): loss=917855.0170265501\n",
      "Gradient Descent(217/999): loss=1460666.898326609\n",
      "Gradient Descent(218/999): loss=893297.9256798753\n",
      "Gradient Descent(219/999): loss=916496.4906416897\n",
      "Gradient Descent(220/999): loss=1443788.9349350359\n",
      "Gradient Descent(221/999): loss=898190.9232524664\n",
      "Gradient Descent(222/999): loss=924705.2136281778\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-356-a2fff3d33fb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train_6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_6\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.03\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-314-5cd6ccddcf28>\u001b[0m in \u001b[0;36mlogistic_regression\u001b[0;34m(y, tx, initial_w, max_iters, gamma)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlogistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient_descent_log_reg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/franckdessimoz/Desktop/Data Science - MA1/Machine Learning/ML_project1/functions.py\u001b[0m in \u001b[0;36mgradient_descent_log_reg\u001b[0;34m(y, tx, initial_w, max_iters, gamma)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_gradient_log_reg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss_log_reg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         print(\"Gradient Descent({bi}/{ti}): loss={l}\".format(\n",
      "\u001b[0;32m/Users/franckdessimoz/Desktop/Data Science - MA1/Machine Learning/ML_project1/functions.py\u001b[0m in \u001b[0;36mcompute_loss_log_reg\u001b[0;34m(h, y)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_loss_log_reg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "w, loss = logistic_regression(y_train, x_train_6, np.zeros(x_train_6.shape[1]), 1000, 0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test_6 = build_poly(x_test_clean, degree = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = zero_to_neg(np.around(sigmoid(x_test_6@w))).astype(int)\n",
    "s = np.column_stack((ids_test.astype(int), prediction.astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_pred(control, prediction):\n",
    "    if(len(control) != len(prediction)):\n",
    "        return 'error'\n",
    "    else:\n",
    "        control = control[control[:,0].argsort()].astype(int)\n",
    "        prediction = prediction[prediction[:,0].argsort()].astype(int)\n",
    "        sum_value = np.sum(np.abs(np.subtract(control[:, 1], prediction[:,1])))/2 \n",
    "        return sum_value, 1 - sum_value/len(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sum_value, perc = test_pred(control, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44867999999999997"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
