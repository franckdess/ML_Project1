{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from proj1_helpers import load_csv_data\n",
    "from functions import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. LOAD THE DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = load_csv_data('Data/train.csv', sub_sample = False)\n",
    "test_set = load_csv_data('Data/test.csv', sub_sample = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1. SET UP THE DATA (LOCAL TESTING)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_set[1]\n",
    "y = train_set[0]\n",
    "ids = train_set[2]\n",
    "\n",
    "x_train, x_test, y_train, y_test, ids_train, ids_test = split_data_tr_te(x, y, ids, 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2. SET UP THE DATA (KAGGLE TESTING)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "x_train = train_set[1]\n",
    "y_train = train_set[0]\n",
    "ids_train = train_set[2]\n",
    "\n",
    "x_test = test_set[1]\n",
    "y_test = test_set[0]\n",
    "ids_test = test_set[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. CLEAN THE DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The following separete the initial test set x_test in three subsets\n",
    "# according the feature PRI_jet_num which takes its value in the \n",
    "# set {0,1,2,3}\n",
    "\n",
    "\n",
    "# Concatenating x, y and ids: \n",
    "\n",
    "complete_test = np.column_stack((y_test, x_test))\n",
    "complete_test = np.column_stack((complete_test, ids_test))\n",
    "\n",
    "# Creating three subsets by the value of the feature PRI_jet_num \n",
    "# which is column 23\n",
    "\n",
    "\n",
    "# We create 3 sub arrays (Subset-0, Subset-1, Subset-23) \n",
    "# according the value of Pri_jet_num which is feature 23\n",
    "\n",
    "subset_test_0 = complete_test[complete_test[:,23] == 0]\n",
    "subset_test_1 = complete_test[complete_test[:,23] == 1]\n",
    "subset_test_23 = complete_test[2 <= complete_test[:,23]]\n",
    "\n",
    "# Separate the subsets by ids_test, y_test and x_test\n",
    "\n",
    "y_test_0 = subset_test_0[:,0]\n",
    "y_test_1 = subset_test_1[:,0]\n",
    "y_test_2 = subset_test_23[:,0]\n",
    "\n",
    "x_test_0 = subset_test_0[:,1:-1]\n",
    "x_test_1 = subset_test_1[:,1:-1]\n",
    "x_test_2 = subset_test_23[:,1:-1]\n",
    "\n",
    "id_test_0 = subset_test_0[:,-1]\n",
    "id_test_1 = subset_test_1[:,-1]\n",
    "id_test_2 = subset_test_23[:,-1]\n",
    "\n",
    "\n",
    "\n",
    "# The following separete the initial training set x in three subsets\n",
    "# according the feature PRI_jet_num which takes its value in the \n",
    "# set {0,1,2,3}\n",
    "\n",
    "\n",
    "# Concatenating x, y and ids: \n",
    "\n",
    "complete_train = np.column_stack((y_train, x_train))\n",
    "complete_train = np.column_stack((complete_train, ids_train))\n",
    "\n",
    "# Creating three subsets by the value of the feature PRI_jet_num \n",
    "# which is column 23\n",
    "\n",
    "\n",
    "# We create 3 sub arrays (Subset-0, Subset-1, Subset-23) \n",
    "# according the value of Pri_jet_num which is feature 23\n",
    "\n",
    "subset_train_0 = complete_train[complete_train[:,23] == 0]\n",
    "subset_train_1 = complete_train[complete_train[:,23] == 1]\n",
    "subset_train_23 = complete_train[2 <= complete_train[:,23]]\n",
    "\n",
    "# Separate the subsets by ids, y and x\n",
    "\n",
    "y_train_0 = subset_train_0[:,0]\n",
    "y_train_1 = subset_train_1[:,0]\n",
    "y_train_2 = subset_train_23[:,0]\n",
    "\n",
    "x_train_0 = subset_train_0[:,1:-1]\n",
    "x_train_1 = subset_train_1[:,1:-1]\n",
    "x_train_2 = subset_train_23[:,1:-1]\n",
    "\n",
    "id_train_0 = subset_train_0[:,-1]\n",
    "id_train_1 = subset_train_1[:,-1]\n",
    "id_train_2 = subset_train_23[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "na_indices_0 = get_na_columns(x_train_0, 0.95, -999)\n",
    "na_indices_1 = get_na_columns(x_train_1, 0.95, -999)\n",
    "na_indices_2 = get_na_columns(x_train_2, 0.95, -999)\n",
    "\n",
    "x_train_0_clean = np.delete(x_train_0, na_indices_0, axis = 1)\n",
    "x_train_1_clean = np.delete(x_train_1, na_indices_1, axis = 1)\n",
    "x_train_2_clean = np.delete(x_train_2, na_indices_2, axis = 1)\n",
    "\n",
    "x_test_0_clean = np.delete(x_test_0, na_indices_0, axis = 1)\n",
    "x_test_1_clean = np.delete(x_test_1, na_indices_1, axis = 1)\n",
    "x_test_2_clean = np.delete(x_test_2, na_indices_2, axis = 1)\n",
    "\n",
    "zero_indices_0 = get_na_columns(x_train_0_clean, 0.99, 0.0)\n",
    "zero_indices_1 = get_na_columns(x_train_1_clean, 0.99, 0.0)\n",
    "zero_indices_2 = get_na_columns(x_train_2_clean, 0.99, 0.0)\n",
    "\n",
    "x_train_0_clean = np.delete(x_train_0_clean, zero_indices_0, axis = 1)\n",
    "x_train_1_clean = np.delete(x_train_1_clean, zero_indices_1, axis = 1)\n",
    "x_train_2_clean = np.delete(x_train_2_clean, zero_indices_2, axis = 1)\n",
    "\n",
    "x_test_0_clean = np.delete(x_test_0_clean, zero_indices_0, axis = 1)\n",
    "x_test_1_clean = np.delete(x_test_1_clean, zero_indices_1, axis = 1)\n",
    "x_test_2_clean = np.delete(x_test_2_clean, zero_indices_2, axis = 1)\n",
    "\n",
    "na_indices_0_rem = get_na_columns(x_train_0_clean, 0, -999)\n",
    "na_indices_1_rem = get_na_columns(x_train_1_clean, 0, -999)\n",
    "na_indices_2_rem = get_na_columns(x_train_2_clean, 0, -999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0], [0], [0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "na_indices_0_rem, na_indices_1_rem, na_indices_2_rem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_0_clean_pred, w_train_0 = predict_na_columns(x_train_0_clean, na_indices_0_rem)\n",
    "x_train_1_clean_pred, w_train_1 = predict_na_columns(x_train_1_clean, na_indices_1_rem)\n",
    "x_train_2_clean_pred, w_train_2 = predict_na_columns(x_train_2_clean, na_indices_2_rem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test_0_clean_pred = set_predict_na_columns(x_test_0_clean, w_train_0, na_indices_0_rem)\n",
    "x_test_1_clean_pred = set_predict_na_columns(x_test_1_clean, w_train_1, na_indices_1_rem)\n",
    "x_test_2_clean_pred = set_predict_na_columns(x_test_2_clean, w_train_2, na_indices_2_rem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_0_std, x_test_0_std = standardize(x_train_0_clean_pred, x_test_0_clean_pred)\n",
    "x_train_0_std_int = np.column_stack((np.ones(x_train_0_std.shape[0]), x_train_0_std))\n",
    "x_test_0_std_int = np.column_stack((np.ones(x_test_0_std.shape[0]), x_test_0_std))\n",
    "\n",
    "x_train_1_std, x_test_1_std = standardize(x_train_1_clean_pred, x_test_1_clean_pred)\n",
    "x_train_1_std_int = np.column_stack((np.ones(x_train_1_std.shape[0]), x_train_1_std))\n",
    "x_test_1_std_int = np.column_stack((np.ones(x_test_1_std.shape[0]), x_test_1_std))\n",
    "\n",
    "x_train_2_std, x_test_2_std = standardize(x_train_2_clean_pred, x_test_2_clean_pred)\n",
    "x_train_2_std_int = np.column_stack((np.ones(x_train_2_std.shape[0]), x_train_2_std))\n",
    "x_test_2_std_int = np.column_stack((np.ones(x_test_2_std.shape[0]), x_test_2_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. DEFINE THE FUNCTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    A = tx.T@tx\n",
    "    b = tx.T@y\n",
    "    w = np.linalg.solve(A, b)\n",
    "    loss = compute_mse(y, tx, w)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    ws = [initial_w]\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        gradient = compute_gradient_least_square(y, tx, w)\n",
    "        loss = compute_mse(y, tx, w)\n",
    "        w = w - gamma*gradient\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def least_squares_SGD(y, tx, initial_w, max_iters, gamma):\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):    \n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size):\n",
    "            gradient = compute_stoch_gradient(minibatch_y, minibatch_tx, w)\n",
    "            loss = compute_mse(minibatch_y, minibatch_tx, w)\n",
    "            new_w = w - gamma*gradient\n",
    "            w = new_w\n",
    "            ws.append(w)\n",
    "            losses.append(loss)\n",
    "    return ws[len(ws)-1], losses[len(losses)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lamb):\n",
    "    aI = lamb * np.identity(tx.shape[1])\n",
    "    a = tx.T.dot(tx) + aI\n",
    "    b = tx.T.dot(y)\n",
    "    w = np.linalg.solve(a, b)\n",
    "    loss = compute_mse(y, tx, w)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression(y, tx, initial_w, max_iters, gamma):\n",
    "    w, loss = gradient_descent_log_reg(y, tx, initial_w, max_iters, gamma)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma):\n",
    "    w, loss = reg_gradient_descent_log_reg(y, tx, lambda_, initial_w, max_iters, gamma)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. GET THE MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_0_2 = build_poly(x_train_0_std_int, degree = 3)\n",
    "x_train_1_2 = build_poly(x_train_1_std_int, degree = 3)\n",
    "x_train_2_2 = build_poly(x_train_2_std_int, degree = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_0, loss_0 = ridge_regression(y_train_0, x_train_0_2, 10**(-11))\n",
    "w_1, loss_1 = ridge_regression(y_train_1, x_train_1_2, 10**(-11))\n",
    "w_2, loss_2 = ridge_regression(y_train_2, x_train_2_2, 10**(-11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. PREDICT Y**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test_0_2 = build_poly(x_test_0_std_int, degree = 3)\n",
    "x_test_1_2 = build_poly(x_test_1_std_int, degree = 3)\n",
    "x_test_2_2 = build_poly(x_test_2_std_int, degree = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_0 = zero_to_neg(np.around(sigmoid(x_test_0_2 @ w_0)))\n",
    "y_1 = zero_to_neg(np.around(sigmoid(x_test_1_2 @ w_1)))\n",
    "y_2 = zero_to_neg(np.around(sigmoid(x_test_2_2 @ w_2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s_0 = np.column_stack((id_test_0, y_0))\n",
    "s_1 = np.column_stack((id_test_1, y_1))\n",
    "s_2 = np.column_stack((id_test_2, y_2))\n",
    "s = np.vstack((np.vstack((s_0, s_1)), s_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = s[s[:,0].argsort()].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. LOCAL TESTING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = np.column_stack((ids_test, y_test)).astype(int)\n",
    "test = test[test[:, 0].argsort()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_pred(y_test, y_pred):\n",
    "    sum_id = np.sum(np.abs(y_test[:, 0] - y_pred[:, 0]))\n",
    "    acc = 1 - np.sum(np.abs(y_test[:, 1] - y_pred[:, 1])) / y_test.shape[0] * 0.5\n",
    "    return sum_id, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sum_id, acc = test_pred(test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0.77819368421052637, 237500)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_id, acc, len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. SAVE THE PREDICTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt('Data/Final/pred_step_4.csv', pred, fmt='%i', delimiter=',', header= \"Id,Prediction\", comments='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
